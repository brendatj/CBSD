{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "09777da7",
      "metadata": {
        "id": "09777da7"
      },
      "source": [
        "# Replicable Machine Learning Model to Determines Respondent Honesty\n",
        "\n",
        "Author :\n",
        "- Gita Rayung Andadari (2041509)\n",
        "- Endrit Sve√ßla (2041500)\n",
        "- Brenda Tellez (2041498)\n",
        "- Michele Zanatta (2045739)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "whjxarl27S3u"
      },
      "id": "whjxarl27S3u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ef7K8-b0FST7",
      "metadata": {
        "id": "ef7K8-b0FST7"
      },
      "source": [
        "Psychologists often use a questionnaire to understand people's behavior based on their responses. Then, the result can be used in many ways, several examples being :\n",
        "\n",
        "1. To determine the degree of someone's fitness in a job environment,\n",
        "2. To determine if a person is ready or not before adopting a child,\n",
        "3. To determine if someone has a specific type of mental disorder,\n",
        "4. To determine if someone is a victim of an accident.\n",
        "\n",
        "However, as these questionnaires' popularity rises, people are becoming more conscious about the relation between their answers and their consequences. Thus, some people fake their answers to get away with the \"best\" consequences. There are two types of faking answers:\n",
        "\n",
        "1. Faking good: Behavior in which subjects present themselves in a **favorable manner**, endorsing desirable traits and rejecting undesirable ones.\n",
        "In example 1, people lie in the job fitness questionnaire so the employer thinks they are a better person, increasing their job acceptance probability.\n",
        "\n",
        "2. Faking bad: Behavior in which subjects present themselves in a **less favorable manner**, endorsing less desirable traits and rejecting desirable ones.\n",
        "\n",
        "In example 3, people lie in the mental disorder questionnaire to receive certain health benefits.\n",
        "\n",
        "Therefore, a system should distinguish whether a response to these questionnaires is honest or dishonest. This problem is a classic binary classification problem from a machine-learning perspective. Traditional machine learning algorithms, such as logistic regression, can be trained to distinguish whether someone is lying with decent accuracy. However, a new problem arose. Every questioner has its own property and a different tendency to fake good or bad. Hence, different machine-learning models had to be used each time. This makes psychologist question the reliability of the model proposed. The ideal method should be **replicable** across different types of datasets. A research study (or, in this project, a machine learning model) is considered replicable when the entire research process is conducted again, using the same methods but new data, and still yields the same results. This shows that the results of the original study are reliable.\n",
        "\n",
        "In this project, our team attempted to find and propose a **replicable machine learning model to distinguish whether a questionnaire response is honest or dishonest**. Our model will be tested against 16 different datasets, 8 faking good and 8 faking bad. We explored three different avenues for our problems:\n",
        "\n",
        "1. Machine learning algorithm paired with model-dependent feature selection ( SelectFromModel() and Forward Selection)\n",
        "2. Model-independent feature selection (Chi-Square, Mutual Information)\n",
        "3. Machine learning algorithm paired with agnostic feature selection (Permutation Importance)\n",
        "4. Machine learning algorithm paired with dimensionality reduction techniques (Principal Component Analysis [PCA], Sparse [PCA] )\n",
        "\n",
        "5 different machine learning algorithms used for the first and third avenues are: Logistic Regression, Random Forest, SVM, and KNN.\n",
        "\n",
        "A method that produces high and stable accuracy across datasets will be considered as the replicable method."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XfvgCzTeFST8",
      "metadata": {
        "id": "XfvgCzTeFST8"
      },
      "source": [
        "We will walk you through the idea that we proposed using one dataset, DT_df_CC.csv\n",
        "Later, we will apply the same method to all 16 datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QmUhgoTJFST8",
      "metadata": {
        "id": "QmUhgoTJFST8"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfc3eeea",
      "metadata": {
        "id": "cfc3eeea"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80287394",
      "metadata": {
        "id": "80287394"
      },
      "outputs": [],
      "source": [
        "# Import dataset from google drive\n",
        "faking_good = {'DT_df_CC.csv' : 'https://drive.google.com/file/d/1DGibOwzy1sXa9wMmbwyzSAVoyyJhV-H9/view?usp=share_link'}\n",
        "\n",
        "file_name = 'DT_df_CC.csv'\n",
        "url = faking_good[file_name]\n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "\n",
        "if file_name.split('.')[1] == 'csv':\n",
        "    df = pd.read_csv(path)\n",
        "    if df.shape[1] == 1:\n",
        "        df = pd.read_csv(path, sep = ';')\n",
        "else:\n",
        "    df = pd.read_excel(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e2a68c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35e2a68c",
        "outputId": "771486b8-e25b-4fff-fdef-19c7860464dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(482, 28)\n",
            "False\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# Checking the shape, null values, and duplicates\n",
        "print(df.shape)\n",
        "print(df.isna().any().any())\n",
        "print(df.duplicated().any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a8ba3f7",
      "metadata": {
        "id": "6a8ba3f7"
      },
      "outputs": [],
      "source": [
        "\"\"\" Function prepare_data: Used to split the data using train_test_split method\n",
        "Input: Dataframe\n",
        "Output: Splited dataframe. \"\"\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "def prepare_data(df):\n",
        "\n",
        "    X = df.loc[:, df.columns != 'CONDITION']\n",
        "    y = df.loc[:, 'CONDITION']\n",
        "    y.replace({'H':1, 'D':0}, inplace=True)\n",
        "\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, train_size = 0.9, random_state=0)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, train_size = 0.8, random_state=0)\n",
        "\n",
        "    return X_train_val, X_test, y_train_val, y_test, X_train, X_val, y_train, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acef8f7e",
      "metadata": {
        "id": "acef8f7e"
      },
      "outputs": [],
      "source": [
        "# Preparing the data\n",
        "X_train_val, X_test, y_train_val, y_test, X_train, X_val, y_train, y_val = prepare_data(df)\n",
        "\n",
        "\n",
        "n_features = X_train_val.shape[1]\n",
        "names = np.array(X_train_val.columns)\n",
        "\n",
        "perc_best_features = 0.2\n",
        "n_best_features = int(perc_best_features*n_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "027db070",
      "metadata": {
        "id": "027db070"
      },
      "source": [
        "# Feature selection model dependent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CDfCtqnCFST-",
      "metadata": {
        "id": "CDfCtqnCFST-"
      },
      "source": [
        "Model-dependent feature selection attempts to perform the selection and the processing of the data simultaneously.\n",
        "\n",
        "Methods:\n",
        "\n",
        "1. Forward Model Selection:\n",
        "\n",
        "Forward Model Selection (FMS) is a feature selection method that starts with an empty set of features and iteratively adds features to the set, one at a time, based on a predefined criterion, such as the improvement in model performance.\n",
        "  \n",
        "The goal of FMS is to find the smallest subset of features that results in the best performance of the model.\n",
        "\n",
        "2. SelectFromModel Method:\n",
        "\n",
        "SelectFromModel is a feature selection method that is implemented as a transformer in the scikit-learn library. It is designed to work with any estimator with a \"coef_\" or \"feature_importances_\" attribute after fitting.\n",
        "  \n",
        "The idea behind SelectFromModel is to use an estimator's coefficients or feature importances to select the most informative features.\n",
        "\n",
        "For each model selection model, we will applied the below procedure:\n",
        "\n",
        "1. Fit the models with 100% of features;\n",
        "2. Apply model dependent features selection method and take the 20% best features. Fit the models with this set of features;\n",
        "3. [just for interpretable models] Select 20% best features based on coefficients values;\n",
        "4. Apply model agnostic, model independent features selection techniques, and dimensionality reduction methods taking the 20% best features. Fit the models with this set of features;\n",
        "5. Compare the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ffcd6eb",
      "metadata": {
        "id": "6ffcd6eb"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from math import exp\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "from sklearn.feature_selection import chi2, SelectPercentile\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e409d7a",
      "metadata": {
        "id": "7e409d7a"
      },
      "outputs": [],
      "source": [
        "\"\"\" Function fit_predict: Is a method that combines the fit and predict methods\n",
        "of a machine learning model into a single function call.\n",
        "Inputs: model: A certain machine learning model\n",
        "        X,y: Train data\n",
        "        X_test,y_test: Test data\n",
        "Output: acc_test_model: Accuracy of the model in the test data.  \"\"\"\n",
        "def fit_predict(model, X, y, X_test, y_test):\n",
        "\n",
        "    model.fit(X, y)\n",
        "\n",
        "    y_train_pred = model.predict(X)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    acc_train_model = accuracy_score(y, y_train_pred)\n",
        "    acc_test_model = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    return acc_test_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99294d35",
      "metadata": {
        "id": "99294d35"
      },
      "outputs": [],
      "source": [
        "\"\"\" Function seq_fit_transform: Implements Forward Model Selection Method as\n",
        "descriped previously.\n",
        "Inputs: model: Machine leaning model\n",
        "        X, y, X_test: Train and test data\n",
        "        n_features_to_select: Number of features to select\n",
        "Output: Selected train and test data. \"\"\"\n",
        "def seq_fit_transform(model, X, y, X_test, n_features_to_select = None):\n",
        "    np.random.seed(0)\n",
        "\n",
        "    sfs = SequentialFeatureSelector(model, direction='forward', cv = 5,\n",
        "                                    n_features_to_select = n_features_to_select)\n",
        "\n",
        "    sfs.fit(X, y)\n",
        "    X_train_selection = sfs.transform(X)\n",
        "    X_test_selection = sfs.transform(X_test)\n",
        "    return X_train_selection, X_test_selection, sfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0KdQ3KauqXnT",
      "metadata": {
        "id": "0KdQ3KauqXnT"
      },
      "outputs": [],
      "source": [
        "\"\"\" Function model_sel: Implements Select From Model Method as\n",
        "descriped previously.\n",
        "Inputs: model: Machine leaning model\n",
        "        X, y, X_test: Train and test data\n",
        "        n_features_to_select: Number of features to select\n",
        "Output: Selected train and test data. \"\"\"\n",
        "def model_sel(model, X, y, X_test, n_features_to_select = None):\n",
        "    np.random.seed(0)\n",
        "\n",
        "    sfs = SelectFromModel(model, max_features = n_features_to_select)\n",
        "\n",
        "    sfs.fit(X, y)\n",
        "    X_train_selection = sfs.transform(X)\n",
        "    X_test_selection = sfs.transform(X_test)\n",
        "    return X_train_selection, X_test_selection, sfs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "738e11c7",
      "metadata": {
        "id": "738e11c7"
      },
      "source": [
        "## Logistic Regression (MD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01449a29",
      "metadata": {
        "id": "01449a29"
      },
      "outputs": [],
      "source": [
        "# Implementation of Logistic Regression with Forward Selection and SelectFromModel methods\n",
        "lr = LogisticRegression()\n",
        "acc_lr_100 = fit_predict(lr, X_train, y_train, X_test, y_test)\n",
        "\n",
        "\n",
        "#forward selection\n",
        "X_train_selection_lr, X_test_selection_lr, sfs_lr = seq_fit_transform(lr, X_train_val, y_train_val, X_test, n_best_features)\n",
        "acc_lr_fw = fit_predict(lr, X_train_selection_lr, y_train_val, X_test_selection_lr, y_test)\n",
        "support_lr_fw= X_train.columns[(sfs_lr.get_support())]\n",
        "\n",
        "#model selection\n",
        "X_train_selection_lr, X_test_selection_lr, sfs_lr = model_sel(lr, X_train_val, y_train_val, X_test, n_best_features)\n",
        "acc_lr_ms = fit_predict(lr, X_train_selection_lr, y_train_val, X_test_selection_lr, y_test)\n",
        "support_lr_ms = X_train.columns[(sfs_lr.get_support())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ANP06VN751yk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANP06VN751yk",
        "outputId": "5dd3c099-c8fd-4558-e817-93ee078d6516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Mach6 ', 'Psycho1 ', 'Psycho5 H', 'Psycho6', 'Psycho8 '], dtype='object')\n",
            "Index(['Mach3 ', 'Mach7 ', 'Psycho1 ', 'Psycho2 ', 'Psycho5 H'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# The features selected from the methods\n",
        "# We observe that 'Psycho1' and 'Psycho5 H' features were picked by both methods\n",
        "print(support_lr_fw)\n",
        "print(support_lr_ms)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64297403",
      "metadata": {
        "id": "64297403"
      },
      "source": [
        "## Random Forest (MD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f7b8e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38f7b8e1",
        "outputId": "8aaae67e-9a8a-4faf-d07a-a7a9b0398511"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=RandomForestClassifier(n_estimators=25),\n",
              "             param_grid={'max_depth': [4, 8, 12],\n",
              "                         'max_leaf_nodes': [None, 3, 5],\n",
              "                         'min_samples_leaf': [1, 3, 5]})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#fine tuning the hyper parameter\n",
        "rf = RandomForestClassifier(n_estimators = 25)\n",
        "\n",
        "parameters_rf = {'max_depth': [4, 8, 12], 'min_samples_leaf' : [1, 3, 5], 'max_leaf_nodes' : [None, 3, 5]}\n",
        "model_rf = GridSearchCV(rf, parameters_rf, cv = 5)\n",
        "model_rf.fit(X_train_val, y_train_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7y6MMqWF8RMZ",
      "metadata": {
        "id": "7y6MMqWF8RMZ"
      },
      "outputs": [],
      "source": [
        "# Implementation of Random Forest Classifier with Forward Selection and SelectFromModel methods\n",
        "model = RandomForestClassifier(n_estimators = 25,\n",
        "                             max_depth = model_rf.best_params_['max_depth'],\n",
        "                             min_samples_leaf = model_rf.best_params_['min_samples_leaf'],\n",
        "                            max_leaf_nodes = model_rf.best_params_['max_leaf_nodes'])\n",
        "\n",
        "rf = model\n",
        "acc_rf_100 = fit_predict(model, X_train_val, y_train_val, X_test, y_test)\n",
        "\n",
        "\n",
        "#forward selection\n",
        "X_train_selection, X_test_selection, sfs = seq_fit_transform(model, X_train_val, y_train_val, X_test, n_best_features)\n",
        "acc_rf_fw = fit_predict(model, X_train_selection, y_train_val, X_test_selection, y_test)\n",
        "support_rf_fw= X_train.columns[(sfs.get_support())]\n",
        "\n",
        "#model selection\n",
        "X_train_selection, X_test_selection, sfs = model_sel(model, X_train_val, y_train_val, X_test, n_best_features)\n",
        "acc_rf_ms = fit_predict(model, X_train_selection, y_train_val, X_test_selection, y_test)\n",
        "support_rf_ms = X_train.columns[(sfs.get_support())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I9QuRUmf3xhU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9QuRUmf3xhU",
        "outputId": "58cfc035-3bd9-4e3e-a3a9-cbb078b23e3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Mach7 ', 'Psycho1 ', 'Psycho5 H', 'Psycho6', 'Narc1'], dtype='object')\n",
            "Index(['Mach7 ', 'Mach9 ', 'Psycho1 ', 'Psycho2 ', 'Psycho5 H'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# The features selected from the methods\n",
        "# We observe that 'Mach7','Psycho1', and 'Psycho5' features were picked by both methods\n",
        "print(support_rf_fw)\n",
        "print(support_rf_ms)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390a2ec9",
      "metadata": {
        "id": "390a2ec9"
      },
      "source": [
        "## SVM (MD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938bfa4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "938bfa4b",
        "outputId": "62dcd51c-d91a-4323-9629-fb58b269f52f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=SVC(),\n",
              "             param_grid={'C': array([0.09531018, 0.40850745, 0.64665336, 0.83885289, 1.        ])})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#fine tuning hyper parameter\n",
        "svm = SVC()\n",
        "C_values = np.log(np.linspace(1.1, exp(1), 5))\n",
        "parameters_svm = {'C' : C_values}\n",
        "23\n",
        "model_svm = GridSearchCV(svm, parameters_svm, cv = 5)\n",
        "model_svm.fit(X_train_val, y_train_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3PJsH4M79Vdg",
      "metadata": {
        "id": "3PJsH4M79Vdg"
      },
      "outputs": [],
      "source": [
        "# Implementation of SVC with Forward Selection method\n",
        "model = SVC(C = model_svm.best_params_['C'])\n",
        "svm = model\n",
        "acc_svm_100 = fit_predict(model, X_train, y_train, X_test, y_test)\n",
        "\n",
        "\n",
        "#forward selection\n",
        "X_train_selection, X_test_selection, sfs = seq_fit_transform(model, X_train_val, y_train_val, X_test, n_best_features)\n",
        "acc_svm_fw = fit_predict(model, X_train_selection, y_train_val, X_test_selection, y_test)\n",
        "support_svm_fw= X_train.columns[(sfs.get_support())]\n",
        "\n",
        "#model selection -- cant be done because the model does not have parameter coefficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VT98YQEq4fKO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT98YQEq4fKO",
        "outputId": "bb82adbc-e8b7-42e7-93fd-45f149a8bc06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Psycho1 ', 'Psycho4 ', 'Psycho5 H', 'Narc7 ', 'Narc8 '], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# The features selected from the method\n",
        "print(support_svm_fw)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f982f69",
      "metadata": {
        "id": "5f982f69"
      },
      "source": [
        "## KNN (MD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "603b1f78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "603b1f78",
        "outputId": "0d4e29e8-3242-43cc-e921-0c18ad874454"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n",
              "             param_grid={'n_neighbors': [1, 5, 10, 20]})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#fine tuning hyper parameter\n",
        "knn = KNeighborsClassifier()\n",
        "parameters_knn = {'n_neighbors' : [1, 5, 10, 20]}\n",
        "\n",
        "model_knn = GridSearchCV(knn, parameters_knn, cv = 5)\n",
        "model_knn.fit(X_train_val, y_train_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KbFBFe9h97ZD",
      "metadata": {
        "id": "KbFBFe9h97ZD"
      },
      "outputs": [],
      "source": [
        "# Implementation of KNN with Forward Selection method\n",
        "model = KNeighborsClassifier(n_neighbors = model_knn.best_params_['n_neighbors'])\n",
        "knn = model\n",
        "acc_knn_100 = fit_predict(model, X_train, y_train, X_test, y_test)\n",
        "\n",
        "\n",
        "#forward selection\n",
        "X_train_selection, X_test_selection, sfs = seq_fit_transform(model, X_train_val, y_train_val, X_test, n_best_features)\n",
        "acc_knn_fw = fit_predict(model, X_train_selection, y_train_val, X_test_selection, y_test)\n",
        "support_knn_fw= X_train.columns[(sfs.get_support())]\n",
        "\n",
        "#model selection -- cant be done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70wGkFs04wEH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70wGkFs04wEH",
        "outputId": "2f7f5fae-f2b7-4b80-9626-dfccb3c2d435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Psycho1 ', 'Psycho2 ', 'Psycho5 H', 'Psycho8 ', 'Narc7 '], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# The features selected from the method\n",
        "print(support_knn_fw)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c21cab62",
      "metadata": {
        "id": "c21cab62"
      },
      "source": [
        "## Comparison (MD)\n",
        "\n",
        "Another"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f456705",
      "metadata": {
        "id": "0f456705"
      },
      "outputs": [],
      "source": [
        "\"\"\" Function jaccard_similarity: Measures the similarity between two sets of data,\n",
        "in this case between two results we obtained from different combinations of models and methods.\n",
        "Input: Two lists of results\n",
        "Output: Jaccard similarity coefficient. \"\"\"\n",
        "def jaccard_similarity(list1, list2):\n",
        "    intersection = len(list(set(list1).intersection(list2)))\n",
        "    union = (len(list1) + len(list2)) - intersection\n",
        "    return float(intersection) / union\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04b6139e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04b6139e",
        "outputId": "3a44d2df-90c4-4bcb-b45e-157a13f4d246"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-4131fd794e2e>:9: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
            "  results_int_fw[i, j] = round(len(support_models_fw[i] & support_models_fw[j])/len(support_models_fw[i]), 2)\n"
          ]
        }
      ],
      "source": [
        "#forward selection\n",
        "support_models_fw = [support_lr_fw, support_rf_fw, support_svm_fw, support_knn_fw]\n",
        "results_js_fw = np.zeros(shape = (len(support_models_fw), len(support_models_fw)))\n",
        "results_int_fw = np.zeros(shape = (len(support_models_fw), len(support_models_fw)))\n",
        "\n",
        "for i in range(len(support_models_fw)):\n",
        "    for j in range(len(support_models_fw)):\n",
        "        results_js_fw[i, j] = round(jaccard_similarity(support_models_fw[i], support_models_fw[j]), 2)\n",
        "        results_int_fw[i, j] = round(len(support_models_fw[i] & support_models_fw[j])/len(support_models_fw[i]), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V0hQkPzzEc_2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0hQkPzzEc_2",
        "outputId": "0341fd41-3248-4e4e-f41a-2b530377ec96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-9f56163a0e25>:9: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
            "  results_int_ms[i, j] = round(len(support_models_ms[i] & support_models_ms[j])/len(support_models_ms[i]), 2)\n"
          ]
        }
      ],
      "source": [
        "#model selection\n",
        "support_models_ms = [support_lr_ms, support_rf_ms]\n",
        "results_js_ms = np.zeros(shape = (len(support_models_ms), len(support_models_ms)))\n",
        "results_int_ms = np.zeros(shape = (len(support_models_ms), len(support_models_ms)))\n",
        "\n",
        "for i in range(len(support_models_ms)):\n",
        "    for j in range(len(support_models_ms)):\n",
        "        results_js_ms[i, j] = round(jaccard_similarity(support_models_ms[i], support_models_ms[j]), 2)\n",
        "        results_int_ms[i, j] = round(len(support_models_ms[i] & support_models_ms[j])/len(support_models_ms[i]), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sk8qmNWcD5qm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk8qmNWcD5qm",
        "outputId": "96479d84-e183-41dc-c263-fc6e747bc9e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jaccard similarity forward selection : \n",
            "        lr    rf   svm   knn\n",
            "lr   1.00  0.43  0.25  0.43\n",
            "rf   0.43  1.00  0.25  0.25\n",
            "svm  0.25  0.25  1.00  0.43\n",
            "knn  0.43  0.25  0.43  1.00\n",
            "\n",
            "\n",
            "Intersection percentage forward selection: \n",
            "       lr   rf  svm  knn\n",
            "lr   1.0  0.6  0.4  0.6\n",
            "rf   0.6  1.0  0.4  0.4\n",
            "svm  0.4  0.4  1.0  0.6\n",
            "knn  0.6  0.4  0.6  1.0\n",
            "\n",
            "\n",
            "Jaccard similarity model selection : \n",
            "       lr    rf\n",
            "lr  1.00  0.67\n",
            "rf  0.67  1.00\n",
            "\n",
            "\n",
            "Intersection percentage model selection: \n",
            "      lr   rf\n",
            "lr  1.0  0.8\n",
            "rf  0.8  1.0\n"
          ]
        }
      ],
      "source": [
        "# Jaccard Similarity results\n",
        "models = ['lr', 'rf', 'svm', 'knn']\n",
        "results1 = pd.DataFrame(results_js_fw, index = models, columns = models)\n",
        "results2 = pd.DataFrame(results_int_fw, index = models, columns = models)\n",
        "print('Jaccard similarity forward selection : \\n', results1)\n",
        "print('\\n')\n",
        "print('Intersection percentage forward selection: \\n', results2)\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "jaccard_fs = results1.mean()\n",
        "int_fs = results2.mean()\n",
        "\n",
        "models = ['lr', 'rf']\n",
        "results1 = pd.DataFrame(results_js_ms, index = models, columns = models)\n",
        "results2 = pd.DataFrame(results_int_ms, index = models, columns = models)\n",
        "print('Jaccard similarity model selection : \\n', results1)\n",
        "print('\\n')\n",
        "print('Intersection percentage model selection: \\n', results2)\n",
        "jaccard_ms = results1.mean()\n",
        "int_ms = results2.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7477538",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7477538",
        "outputId": "f8adb091-787a-4b2e-8263-46fea5a69d06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             lr        rf       svm       knn\n",
            "100%                   0.734694  0.755102  0.755102  0.693878\n",
            "20% forward selection   0.77551  0.653061  0.693878  0.714286\n",
            "20% model selection    0.755102  0.714286      None      None\n"
          ]
        }
      ],
      "source": [
        "# Accuracy results\n",
        "acc_100 = [acc_lr_100, acc_rf_100, acc_svm_100, acc_knn_100]\n",
        "acc_20_fw = [acc_lr_fw, acc_rf_fw, acc_svm_fw, acc_knn_fw]\n",
        "acc_20_ms = [acc_lr_ms, acc_rf_ms, None, None]\n",
        "\n",
        "acc_comp = np.array([acc_100, acc_20_fw, acc_20_ms])\n",
        "\n",
        "models = ['lr','rf','svm','knn']\n",
        "results_accuracy = pd.DataFrame(acc_comp, columns = models, index = ['100%', '20% forward selection', '20% model selection'])\n",
        "\n",
        "print(results_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d6187fb",
      "metadata": {
        "id": "8d6187fb"
      },
      "source": [
        "# Model independent feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd39CrNQFSUB",
      "metadata": {
        "id": "bd39CrNQFSUB"
      },
      "source": [
        "Model independent feature selection is a feature selection method where features are selected based on a correlation type statistical measures between input and output variables. In this project, two model independent feature selection are being considered:\n",
        "\n",
        "1. Chi Square\n",
        "2. Mutual Information\n",
        "\n",
        "The procedure for model independent feature selection are:\n",
        "\n",
        "1. Fit the models with 100% of features;\n",
        "2. Calculate the correlation type statistical measures between input and output;\n",
        "3. Select 20% best features based on metric in step number 2;\n",
        "4. Train the machine learning model just using the selected features\n",
        "5. Compare the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xrys1nSQJ5FB",
      "metadata": {
        "id": "xrys1nSQJ5FB"
      },
      "source": [
        "## Chi Squared Filter Method\n",
        "\n",
        "We calculate Chi-square between each feature and the target and select the desired number of features with best Chi-square scores.\n",
        "It determines if the association between two categorical variables of the sample would reflect their real association in the population."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93cb2ef1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93cb2ef1",
        "outputId": "8ec54bb5-9efe-4d85-e248-9cb5bc2a4d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chi support ['Mach5 ' 'Psycho1 ' 'Psycho2 ' 'Psycho5 H' 'Psycho8 ']\n"
          ]
        }
      ],
      "source": [
        "# Chi square method and the features that were selected\n",
        "\n",
        "model_chi2 = SelectKBest(chi2, k=5)\n",
        "\n",
        "model_chi2.fit(X_train_val, y_train_val)\n",
        "support_chi2 = model_chi2.get_support()\n",
        "\n",
        "X_train_selected_chi2 = model_chi2.transform(X_train_val)\n",
        "X_test_selected_chi2 = model_chi2.transform(X_test)\n",
        "chi_support = names[support_chi2]\n",
        "\n",
        "print('Chi support {}'.format(chi_support))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a2d52aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a2d52aa",
        "outputId": "8c9fa334-84c4-4012-b971-1d74180ef338"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-27-2cff58755c86>:9: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
            "  results_int[i] = round(len(support_models_fw[i] & chi_support)/len(chi_support), 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jaccard similarity between forward selection and chi square filter method: \n",
            "      Chi2\n",
            "lr   0.43\n",
            "rf   0.25\n",
            "svm  0.25\n",
            "knn  0.67\n",
            "\n",
            "\n",
            "Intersection percentage between forward selection and chi square filter method: \n",
            "      Chi2\n",
            "lr    0.6\n",
            "rf    0.4\n",
            "svm   0.4\n",
            "knn   0.8\n",
            "\n",
            "\n",
            "Jaccard similarity between model selection and chi square filter method: \n",
            "     Chi2\n",
            "lr  0.43\n",
            "rf  0.43\n",
            "\n",
            "\n",
            "Intersection percentage between model selection and chi square filter method: \n",
            "     Chi2\n",
            "lr   0.6\n",
            "rf   0.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-27-2cff58755c86>:26: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
            "  results_int[i] = round(len(support_models_ms[i] & chi_support)/len(chi_support), 2)\n"
          ]
        }
      ],
      "source": [
        "# Jaccard Similarity between the methods\n",
        "\n",
        "results_js = np.zeros(shape = (len(support_models_fw), 1))\n",
        "results_int = np.zeros(shape = (len(support_models_fw), 1))\n",
        "\n",
        "\n",
        "for i in range(len(support_models_fw)):\n",
        "        results_js[i] = round(jaccard_similarity(support_models_fw[i], chi_support), 2)\n",
        "        results_int[i] = round(len(support_models_fw[i] & chi_support)/len(chi_support), 2)\n",
        "\n",
        "models = ['lr', 'rf', 'svm', 'knn']\n",
        "results5 = pd.DataFrame(results_js, index = models, columns = ['Chi2'])\n",
        "results6 = pd.DataFrame(results_int, index = models, columns = ['Chi2'])\n",
        "print('Jaccard similarity between forward selection and chi square filter method: \\n', results5)\n",
        "print('\\n')\n",
        "print('Intersection percentage between forward selection and chi square filter method: \\n', results6)\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "results_js = np.zeros(shape = (len(support_models_ms), 1))\n",
        "results_int = np.zeros(shape = (len(support_models_ms), 1))\n",
        "\n",
        "\n",
        "for i in range(len(support_models_ms)):\n",
        "        results_js[i] = round(jaccard_similarity(support_models_ms[i], chi_support), 2)\n",
        "        results_int[i] = round(len(support_models_ms[i] & chi_support)/len(chi_support), 2)\n",
        "\n",
        "models = ['lr', 'rf']\n",
        "results5 = pd.DataFrame(results_js, index = models, columns = ['Chi2'])\n",
        "results6 = pd.DataFrame(results_int, index = models, columns = ['Chi2'])\n",
        "print('Jaccard similarity between model selection and chi square filter method: \\n', results5)\n",
        "print('\\n')\n",
        "print('Intersection percentage between model selection and chi square filter method: \\n', results6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AvDq9ZKeFSUC",
      "metadata": {
        "id": "AvDq9ZKeFSUC"
      },
      "outputs": [],
      "source": [
        "#train the model with reduce feature\n",
        "acc_lr3 = fit_predict(lr, X_train_selected_chi2, y_train_val, X_test_selected_chi2, y_test)\n",
        "\n",
        "rf3 = rf\n",
        "acc_rf3 = fit_predict(rf3, X_train_selected_chi2, y_train_val, X_test_selected_chi2, y_test)\n",
        "\n",
        "svm3 = svm\n",
        "acc_svm3 = fit_predict(svm3, X_train_selected_chi2, y_train_val, X_test_selected_chi2, y_test)\n",
        "\n",
        "knn3 = knn\n",
        "acc_knn3 = fit_predict(knn3, X_train_selected_chi2, y_train_val, X_test_selected_chi2, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f6Awk5MFSUC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f6Awk5MFSUC",
        "outputId": "bcb137e8-cfbe-4621-a103-075eb6d3bda9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             lr        rf       svm       knn\n",
            "100%                   0.734694  0.755102  0.755102  0.693878\n",
            "20% forward selection   0.77551  0.653061  0.693878  0.714286\n",
            "20% model selection    0.755102  0.714286      None      None\n",
            "chi2                   0.734694  0.755102  0.734694   0.77551\n"
          ]
        }
      ],
      "source": [
        "acc_chi2 = [acc_lr3, acc_rf3, acc_svm3, acc_knn3]\n",
        "\n",
        "acc_comp = np.array([acc_100, acc_20_fw, acc_20_ms, acc_chi2])\n",
        "\n",
        "models = ['lr', 'rf', 'svm', 'knn']\n",
        "results_accuracy = pd.DataFrame(acc_comp, columns = models, index = ['100%', '20% forward selection', '20% model selection','chi2'])\n",
        "\n",
        "print(results_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aPXxfUZxFSUC",
      "metadata": {
        "id": "aPXxfUZxFSUC"
      },
      "source": [
        "## Mutual Information\n",
        "\n",
        "The entropy of a random variable is the average level of \"information,\" \"surprise,\" or \"uncertainty.\"\n",
        "\n",
        "Mutual Information (Information Gain)  calculates the statistical dependence between two variables to find the reduction in entropy from transforming a dataset somehow.\n",
        "\n",
        "Feature selection: by evaluating the gain of each variable in the context of the target variable and picking the ones with the most information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EMBWA1kOFSUC",
      "metadata": {
        "id": "EMBWA1kOFSUC"
      },
      "outputs": [],
      "source": [
        "# feature selection - mutual information\n",
        "fs = SelectKBest(score_func=mutual_info_classif, k=n_best_features)\n",
        "fs.fit(X_train_val, y_train_val)\n",
        "\n",
        "X_train_mi = SelectKBest(mutual_info_classif, k=6).fit_transform(X_train_val,y_train_val)\n",
        "X_test_mi = SelectKBest(mutual_info_classif, k=6).fit_transform(X_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7sHQLVS8FSUC",
      "metadata": {
        "id": "7sHQLVS8FSUC"
      },
      "outputs": [],
      "source": [
        "#train the model with reduce feature\n",
        "acc_lr_mi = fit_predict(lr, X_train_mi, y_train_val, X_test_mi, y_test)\n",
        "\n",
        "rf3 = rf\n",
        "acc_rf_mi = fit_predict(rf, X_train_mi, y_train_val, X_test_mi, y_test)\n",
        "\n",
        "svm3 = svm\n",
        "acc_svm_mi = fit_predict(svm, X_train_mi, y_train_val, X_test_mi, y_test)\n",
        "\n",
        "knn3 = knn\n",
        "acc_knn_mi = fit_predict(knn, X_train_mi, y_train_val, X_test_mi, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3Ful_W3KFSUC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ful_W3KFSUC",
        "outputId": "4dede545-b242-4cae-bf5a-9ae563b2b0e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             lr        rf       svm       knn\n",
            "100%                   0.734694  0.755102  0.755102  0.693878\n",
            "20% forward selection   0.77551  0.653061  0.693878  0.714286\n",
            "20% model selection    0.755102  0.714286      None      None\n",
            "chi2                   0.734694  0.755102  0.734694   0.77551\n",
            "Mutual Info.           0.591837  0.734694  0.734694  0.693878\n"
          ]
        }
      ],
      "source": [
        "acc_mi = [acc_lr_mi, acc_rf_mi, acc_svm_mi, acc_knn_mi]\n",
        "\n",
        "acc_comp = np.array([acc_100, acc_20_fw, acc_20_ms, acc_chi2, acc_mi])\n",
        "\n",
        "models = ['lr', 'rf', 'svm', 'knn']\n",
        "results_accuracy = pd.DataFrame(acc_comp, columns = models, index = ['100%', '20% forward selection', '20% model selection','chi2' ,'Mutual Info.'])\n",
        "#results_accuracy.index.name = file_name\n",
        "print(results_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OLLuyZuvFSUC",
      "metadata": {
        "id": "OLLuyZuvFSUC"
      },
      "source": [
        "# Model agnostic feature selection\n",
        "\n",
        "Model agnostic methods refer to a broader category of machine learning methods that do not rely on specific assumptions about the underlying distribution or structure of the data or the specific characteristics of the model being used.\n",
        "\n",
        "These methods can be applied to a wide variety of data and models, and they are often considered to be more flexible and generalizable than model-specific methods.\n",
        "\n",
        "Model agnostic separates the explanations from the machine learning model. The advantages of model-agnostic interpretation methods are:\n",
        "\n",
        "1. Model flexibility\n",
        "2. Explanation flexibility\n",
        "3. Representation flexibility"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "juZyF62VFSUC",
      "metadata": {
        "id": "juZyF62VFSUC"
      },
      "source": [
        "## Feature permutation\n",
        "\n",
        "\n",
        "Permutation feature importance measures the increase in the model‚Äôs prediction error after we permuted the feature‚Äôs values, which breaks the relationship between the feature and the true outcome.\n",
        "\n",
        "A feature is ‚Äúimportant‚Äù if shuffling its values increases the model error because, in this case, the model relied on the feature for the prediction.\n",
        "\n",
        "A feature is ‚Äúunimportant‚Äù if shuffling its values leaves the model error unchanged because, in this case, the model ignored the feature for the prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rKkAUAmqFSUD",
      "metadata": {
        "id": "rKkAUAmqFSUD"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yZZjKhMrFSUD",
      "metadata": {
        "id": "yZZjKhMrFSUD"
      },
      "outputs": [],
      "source": [
        "\"\"\" Function imporance_permutation: Implements Permutation Importance feature selection method\n",
        "previous discussed together with a ML model.\n",
        "Inputs: model: Machine learning model\n",
        "        X_train_val: Training data\n",
        "        perc_features_selected\n",
        "Output: Features selected. \"\"\"\n",
        "def importance_permutation(model, X_train_val, perc_features_selected = None):\n",
        "    n_features = X_train_val.shape[1]\n",
        "    features_selected = np.full((n_features), False)\n",
        "\n",
        "    if perc_features_selected:\n",
        "        number_features = int(perc_features_selected*n_features)\n",
        "        for i in model.importances_mean.argsort()[::-1][:number_features]:\n",
        "            features_selected[i] = True\n",
        "            #print(f\"{names[i]:<8}\\t\"\n",
        "            #     f\"{model.importances_mean[i]:.3f}\"\n",
        "            #     f\" +/- {model.importances_std[i]:.3f}\")\n",
        "    else:\n",
        "        for i in model.importances_mean.argsort()[::-1]:\n",
        "            if model.importances_mean[i] - 2 * model.importances_std[i] > 0:\n",
        "                features_selected[i] = True\n",
        "            #    print(f\"{names[i]:<8}\\t\"\n",
        "            #         f\"{model.importances_mean[i]:.3f}\"\n",
        "            #         f\" +/- {model.importances_std[i]:.3f}\")\n",
        "\n",
        "    return features_selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dahOWRpgFSUD",
      "metadata": {
        "id": "dahOWRpgFSUD"
      },
      "outputs": [],
      "source": [
        "# Execution of the method with each model\n",
        "# Logistic Regression\n",
        "lr4 = lr\n",
        "lr4.fit(X_train_val, y_train_val)\n",
        "lr4_pi = permutation_importance(lr4, X_train_val, y_train_val,\n",
        "                           n_repeats=30,\n",
        "                           random_state=0)\n",
        "\n",
        "perm_lr = importance_permutation(lr4_pi,X_train_val,perc_features_selected = perc_best_features)\n",
        "\n",
        "X_train_selected_lr_perm = X_train_val.loc[:, names[perm_lr]]\n",
        "X_test_selected_lr_perm = X_test.loc[:, names[perm_lr]]\n",
        "\n",
        "#train the model with reduced features\n",
        "lr4 = lr\n",
        "acc_lr4 = fit_predict(lr4, X_train_selected_lr_perm, y_train_val, X_test_selected_lr_perm, y_test)\n",
        "\n",
        "# Random Forest\n",
        "rf4 = rf\n",
        "rf4.fit(X_train_val, y_train_val)\n",
        "rf4_pi = permutation_importance(rf4, X_train_val, y_train_val,\n",
        "                           n_repeats=30,\n",
        "                           random_state=0)\n",
        "\n",
        "perm_rf = importance_permutation(rf4_pi,X_train_val,perc_features_selected = perc_best_features)\n",
        "\n",
        "X_train_selected_rf_perm = X_train_val.loc[:, names[perm_rf]]\n",
        "X_test_selected_rf_perm = X_test.loc[:, names[perm_rf]]\n",
        "\n",
        "rf4 = rf\n",
        "\n",
        "acc_rf4 = fit_predict(rf4, X_train_selected_rf_perm, y_train_val, X_test_selected_rf_perm, y_test)\n",
        "\n",
        "# SVM\n",
        "svm4 = svm\n",
        "\n",
        "svm4.fit(X_train_val, y_train_val)\n",
        "svm4_pi = permutation_importance(svm4, X_train_val, y_train_val,\n",
        "                           n_repeats=30,\n",
        "                           random_state=0)\n",
        "\n",
        "perm_svm = importance_permutation(svm4_pi,X_train_val,perc_features_selected = perc_best_features)\n",
        "\n",
        "X_train_selected_svm_perm = X_train_val.loc[:, names[perm_svm]]\n",
        "X_test_selected_svm_perm = X_test.loc[:, names[perm_svm]]\n",
        "\n",
        "svm4 = svm\n",
        "\n",
        "acc_svm4 = fit_predict(svm4, X_train_selected_svm_perm, y_train_val, X_test_selected_svm_perm, y_test)\n",
        "\n",
        "# KNN\n",
        "knn4 = knn\n",
        "\n",
        "knn4.fit(X_train_val, y_train_val)\n",
        "knn4_pi = permutation_importance(knn4, X_train_val, y_train_val,\n",
        "                           n_repeats=30,\n",
        "                           random_state=0)\n",
        "\n",
        "perm_knn = importance_permutation(knn4_pi,X_train_val,perc_features_selected = perc_best_features)\n",
        "\n",
        "X_train_selected_knn_perm = X_train_val.loc[:, names[perm_knn]]\n",
        "X_test_selected_knn_perm = X_test.loc[:, names[perm_knn]]\n",
        "\n",
        "knn4 = knn\n",
        "\n",
        "acc_knn4 = fit_predict(knn4, X_train_selected_knn_perm, y_train_val, X_test_selected_knn_perm, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JhtkWdkwFSUC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhtkWdkwFSUC",
        "outputId": "faf73127-93bd-490e-c1c5-e36fbeda8b93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jaccard similarity: \n",
            "        lr    rf   svm   knn\n",
            "lr   1.00  0.67  0.67  0.43\n",
            "rf   0.67  1.00  0.67  0.25\n",
            "svm  0.67  0.67  1.00  0.25\n",
            "knn  0.43  0.25  0.25  1.00\n",
            "\n",
            "\n",
            "Intersection percentage: \n",
            "       lr   rf  svm  knn\n",
            "lr   1.0  0.8  0.8  0.6\n",
            "rf   0.8  1.0  0.8  0.4\n",
            "svm  0.8  0.8  1.0  0.4\n",
            "knn  0.6  0.4  0.4  1.0\n"
          ]
        }
      ],
      "source": [
        "# Jaccard similarity results from permutation importance method\n",
        "support_models3 = [perm_lr, perm_rf, perm_svm, perm_knn]\n",
        "\n",
        "results_js = np.zeros(shape = (len(support_models3), len(support_models3)))\n",
        "results_int = np.zeros(shape = (len(support_models3), len(support_models3)))\n",
        "\n",
        "for i in range(len(support_models3)):\n",
        "    for j in range(len(support_models3)):\n",
        "        results_js[i, j] = round(jaccard_similarity(names[support_models3[i]], names[support_models3[j]]), 2)\n",
        "        results_int[i, j] = round(len(names[support_models3[i] & support_models3[j]])/len(names[support_models3[i]]), 2)\n",
        "\n",
        "models = ['lr', 'rf', 'svm', 'knn']\n",
        "results7 = pd.DataFrame(results_js, index = models, columns = models)\n",
        "results8 = pd.DataFrame(results_int, index = models, columns = models)\n",
        "jaccard_pi = results7.mean()\n",
        "int_pi = results7.mean()\n",
        "print('Jaccard similarity: \\n', results7)\n",
        "print('\\n')\n",
        "print('Intersection percentage: \\n', results8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ygZI0m_cFSUD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygZI0m_cFSUD",
        "outputId": "4024433b-1419-4380-b56b-7dd906f33c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    lr        rf       svm       knn\n",
            "100%          0.734694  0.755102  0.755102  0.693878\n",
            "20% FW         0.77551  0.653061  0.693878  0.714286\n",
            "20% ms        0.755102  0.714286      None      None\n",
            "chi2          0.734694  0.755102  0.734694   0.77551\n",
            "Mutual info.  0.591837  0.734694  0.734694  0.693878\n",
            "Perm_imp      0.755102  0.734694  0.734694  0.734694\n"
          ]
        }
      ],
      "source": [
        "# Accuracies of the models implemented until now\n",
        "acc_perm = [acc_lr4, acc_rf4, acc_svm4, acc_knn4]\n",
        "\n",
        "acc_comp = np.array([acc_100, acc_20_fw, acc_20_ms, acc_chi2,acc_mi, acc_perm])\n",
        "\n",
        "results_accuracy = pd.DataFrame(acc_comp, columns = models, index = ['100%', '20% FW', '20% ms','chi2','Mutual info.' ,'Perm_imp'])\n",
        "\n",
        "print(results_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fgY0NRtJ4uW",
      "metadata": {
        "id": "4fgY0NRtJ4uW"
      },
      "source": [
        "# Dimensionality Reduction Techniques\n",
        "\n",
        "Dimensionality reduction is a technique used to reduce the number of features or variables in a dataset. It is often used to remove redundant or irrelevant features, improve the interpretability of the data, and speed up the training of machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31X84jzU0vUb",
      "metadata": {
        "id": "31X84jzU0vUb"
      },
      "source": [
        "## PCA\n",
        "\n",
        "It is a linear dimensionality reduction technique that aims to find a new set of uncorrelated variables, called principal components, that best capture the variation in the original data. It is often used to visualize high-dimensional data in lower-dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4UT59DZ_0xKh",
      "metadata": {
        "id": "4UT59DZ_0xKh"
      },
      "outputs": [],
      "source": [
        "# Initiating sklearn PCA\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "pca = PCA(n_components = n_best_features)\n",
        "# Fit PCA in the train set\n",
        "pca.fit(X_train_val)\n",
        "\n",
        "# Transform train and test set\n",
        "\n",
        "X_train_pca = pca.transform(X_train_val)\n",
        "X_test_pca = pca.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kK_DXZmo7LwD",
      "metadata": {
        "id": "kK_DXZmo7LwD"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe from the transformed data\n",
        "#X_train\n",
        "X_train_pca_df = pd.DataFrame(X_train_pca, columns=['PC{}'.format(i+1) for i in range(X_train_pca.shape[1])])\n",
        "\n",
        "#X_test\n",
        "X_test_pca_df = pd.DataFrame(X_test_pca, columns=['PC{}'.format(i+1) for i in range(X_test_pca.shape[1])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S826An25B2pJ",
      "metadata": {
        "id": "S826An25B2pJ"
      },
      "outputs": [],
      "source": [
        "#train the models with reduced features\n",
        "lr5 = lr\n",
        "acc_lr_pca = fit_predict(lr5, X_train_pca_df, y_train_val, X_test_pca_df, y_test)\n",
        "\n",
        "rf5 = rf\n",
        "acc_rf_pca = fit_predict(rf5, X_train_pca_df, y_train_val, X_test_pca_df, y_test)\n",
        "\n",
        "svm5 = svm\n",
        "acc_svm_pca = fit_predict(svm5, X_train_pca_df, y_train_val, X_test_pca_df, y_test)\n",
        "\n",
        "knn5 = knn\n",
        "acc_knn_pca = fit_predict(knn5, X_train_pca_df, y_train_val, X_test_pca_df, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfJ15WTYHmlV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfJ15WTYHmlV",
        "outputId": "2b41141f-2dd7-4ee3-8631-1c57354d0a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                              lr        rf       svm       knn\n",
            "100%                    0.734694  0.755102  0.755102  0.693878\n",
            "20% forward selection    0.77551  0.653061  0.693878  0.714286\n",
            "20% model selection     0.755102  0.714286      None      None\n",
            "chi2                    0.734694  0.755102  0.734694   0.77551\n",
            "permutation importance  0.755102  0.734694  0.734694  0.734694\n",
            "mutual information      0.591837  0.734694  0.734694  0.693878\n",
            "PCA                      0.77551   0.77551  0.734694  0.734694\n"
          ]
        }
      ],
      "source": [
        "#Accuracies\n",
        "acc_pca = [acc_lr_pca, acc_rf_pca, acc_svm_pca, acc_knn_pca]\n",
        "\n",
        "acc_comp = np.array([acc_100, acc_20_fw, acc_20_ms, acc_chi2, acc_perm, acc_mi,acc_pca])\n",
        "\n",
        "models = ['lr', 'rf', 'svm', 'knn']\n",
        "results_accuracy = pd.DataFrame(acc_comp, columns = models, index = ['100%', '20% forward selection', '20% model selection','chi2', 'permutation importance','mutual information','PCA'])\n",
        "#results_accuracy.index.name = file_name\n",
        "print(results_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fjmJ9qgTs72q",
      "metadata": {
        "id": "fjmJ9qgTs72q"
      },
      "source": [
        "## Sparse PCA\n",
        "\n",
        "It is a variant of PCA that aims to find the directions in the data that have the highest variance and project the data onto those directions while maintaining the sparsity of the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TzVjjQy9s7GH",
      "metadata": {
        "id": "TzVjjQy9s7GH"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import SparsePCA\n",
        "\n",
        "# Initialize the model with the desired number of components\n",
        "sparse_pca = SparsePCA(n_components = n_best_features, alpha=0.01)\n",
        "\n",
        "\n",
        "# fit the train set and transform train and test set\n",
        "sparse_pca.fit(X_train_val)\n",
        "X_train_spca = sparse_pca.transform(X_train_val)\n",
        "X_test_spca = sparse_pca.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9jQQa2xQtDTe",
      "metadata": {
        "id": "9jQQa2xQtDTe"
      },
      "outputs": [],
      "source": [
        "#X_train\n",
        "X_train_spca_df = pd.DataFrame(X_train_spca, columns=['PC{}'.format(i+1) for i in range(X_train_spca.shape[1])])\n",
        "\n",
        "#X_test\n",
        "X_test_spca_df = pd.DataFrame(X_test_spca, columns=['PC{}'.format(i+1) for i in range(X_test_spca.shape[1])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4hwSc_0qx02Q",
      "metadata": {
        "id": "4hwSc_0qx02Q"
      },
      "outputs": [],
      "source": [
        "#train the models with reduced features\n",
        "lr6 = lr\n",
        "acc_lr_spca = fit_predict(lr6, X_train_spca_df, y_train_val, X_test_spca_df, y_test)\n",
        "\n",
        "rf6 = rf\n",
        "acc_rf_spca = fit_predict(rf6, X_train_spca_df, y_train_val, X_test_spca_df, y_test)\n",
        "\n",
        "svm6 = svm\n",
        "acc_svm_spca = fit_predict(svm6, X_train_spca_df, y_train_val, X_test_spca_df, y_test)\n",
        "\n",
        "knn6 = knn\n",
        "acc_knn_spca = fit_predict(knn6, X_train_spca_df, y_train_val, X_test_spca_df, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s1b3RwjkJxpy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1b3RwjkJxpy",
        "outputId": "3ff24f63-168b-42b2-bd92-c5bdbb3fa60f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                              lr        rf       svm       knn     file_name\n",
            "100%                    0.734694  0.755102  0.755102  0.693878  DT_df_CC.csv\n",
            "20% forward selection    0.77551  0.653061  0.693878  0.714286  DT_df_CC.csv\n",
            "20% model selection     0.755102  0.714286      None      None  DT_df_CC.csv\n",
            "chi2                    0.734694  0.755102  0.734694   0.77551  DT_df_CC.csv\n",
            "permutation importance  0.755102  0.734694  0.734694  0.734694  DT_df_CC.csv\n",
            "mutual information      0.591837  0.734694  0.734694  0.693878  DT_df_CC.csv\n",
            "PCA                      0.77551   0.77551  0.734694  0.734694  DT_df_CC.csv\n",
            "Sparse PCA               0.77551  0.734694  0.734694   0.77551  DT_df_CC.csv\n"
          ]
        }
      ],
      "source": [
        "# Accuracies\n",
        "acc_spca = [acc_lr_spca, acc_rf_spca, acc_svm_spca, acc_knn_spca]\n",
        "\n",
        "acc_comp = np.array([acc_100, acc_20_fw, acc_20_ms, acc_chi2, acc_perm, acc_mi,acc_pca, acc_spca])\n",
        "\n",
        "models = ['lr', 'rf', 'svm', 'knn']\n",
        "results_accuracy = pd.DataFrame(acc_comp, columns = models, index = ['100%', '20% forward selection', '20% model selection','chi2', 'permutation importance','mutual information','PCA', 'Sparse PCA'])\n",
        "results_accuracy['file_name'] = file_name\n",
        "\n",
        "#results_accuracy.index.name = file_name\n",
        "print(results_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bwXbhxPRFSUE",
      "metadata": {
        "id": "bwXbhxPRFSUE"
      },
      "source": [
        "# Repeat to all dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0T4gsT7DFSUE",
      "metadata": {
        "id": "0T4gsT7DFSUE"
      },
      "outputs": [],
      "source": [
        "\"\"\" The following function repeatability: Repeats the procedure to all the other 15 datasets.\n",
        "Input: file_name: Name of the file\n",
        "       path: File path \"\"\"\n",
        "def repeatability(file_name,path):\n",
        "    results_accuracy = pd.DataFrame()\n",
        "\n",
        "    if file_name.split('.')[1] == 'csv':\n",
        "        df = pd.read_csv(path)\n",
        "        if df.shape[1] == 1:\n",
        "            df = pd.read_csv(path, sep = ';')\n",
        "\n",
        "    X_train_val, X_test, y_train_val, y_test, X_train, X_val, y_train, y_val = prepare_data(df)\n",
        "\n",
        "    n_features = X_train_val.shape[1]\n",
        "    names = np.array(X_train_val.columns)\n",
        "\n",
        "    perc_best_features = 0.2\n",
        "    n_best_features = int(perc_best_features*n_features)\n",
        "\n",
        "    ########################\n",
        "    # Model dependent - LR #\n",
        "    ########################\n",
        "    lr = LogisticRegression()\n",
        "    acc_lr_100 = fit_predict(lr, X_train, y_train, X_test, y_test)\n",
        "\n",
        "    #forward selection\n",
        "    X_train_selection_lr, X_test_selection_lr, sfs_lr = seq_fit_transform(lr, X_train_val, y_train_val, X_test, n_best_features)\n",
        "    acc_lr_fw = fit_predict(lr, X_train_selection_lr, y_train_val, X_test_selection_lr, y_test)\n",
        "    support_lr_fw= X_train.columns[(sfs_lr.get_support())]\n",
        "\n",
        "    #model selection\n",
        "    X_train_selection_lr, X_test_selection_lr, sfs_lr = model_sel(lr, X_train_val, y_train_val, X_test, n_best_features)\n",
        "    acc_lr_ms = fit_predict(lr, X_train_selection_lr, y_train_val, X_test_selection_lr, y_test)\n",
        "    support_lr_ms = X_train.columns[(sfs_lr.get_support())]\n",
        "\n",
        "    ########################\n",
        "    # Model dependent - RF #\n",
        "    ########################\n",
        "    #fine tuning the hyper parameter\n",
        "    rf = RandomForestClassifier(n_estimators = 25)\n",
        "\n",
        "    parameters_rf = {'max_depth': [4, 8, 12], 'min_samples_leaf' : [1, 3, 5], 'max_leaf_nodes' : [None, 3, 5]}\n",
        "    model_rf = GridSearchCV(rf, parameters_rf, cv = 5)\n",
        "    model_rf.fit(X_train_val, y_train_val)\n",
        "\n",
        "    model = RandomForestClassifier(n_estimators = 25,\n",
        "                                 max_depth = model_rf.best_params_['max_depth'],\n",
        "                                 min_samples_leaf = model_rf.best_params_['min_samples_leaf'],\n",
        "                                max_leaf_nodes = model_rf.best_params_['max_leaf_nodes'])\n",
        "\n",
        "    rf = model\n",
        "    acc_rf_100 = fit_predict(model, X_train_val, y_train_val, X_test, y_test)\n",
        "\n",
        "\n",
        "    #forward selection\n",
        "    X_train_selection, X_test_selection, sfs = seq_fit_transform(model, X_train_val, y_train_val, X_test, n_best_features)\n",
        "    acc_rf_fw = fit_predict(model, X_train_selection, y_train_val, X_test_selection, y_test)\n",
        "    support_rf_fw= X_train.columns[(sfs.get_support())]\n",
        "\n",
        "    #model selection\n",
        "    X_train_selection, X_test_selection, sfs = model_sel(model, X_train_val, y_train_val, X_test, n_best_features)\n",
        "    acc_rf_ms = fit_predict(model, X_train_selection, y_train_val, X_test_selection, y_test)\n",
        "    support_rf_ms = X_train.columns[(sfs.get_support())]\n",
        "\n",
        "    ########################\n",
        "    # Model dependent - SVM #\n",
        "    ########################\n",
        "\n",
        "    #fine tuning hyper parameter\n",
        "    svm = SVC()\n",
        "    C_values = np.log(np.linspace(1.1, exp(1), 5))\n",
        "    parameters_svm = {'C' : C_values}\n",
        "    23\n",
        "    model_svm = GridSearchCV(svm, parameters_svm, cv = 5)\n",
        "    model_svm.fit(X_train_val, y_train_val)\n",
        "\n",
        "    model = SVC(C = model_svm.best_params_['C'])\n",
        "    svm = model\n",
        "    acc_svm_100 = fit_predict(model, X_train, y_train, X_test, y_test)\n",
        "\n",
        "\n",
        "    #forward selection\n",
        "    X_train_selection, X_test_selection, sfs = seq_fit_transform(model, X_train_val, y_train_val, X_test, n_best_features)\n",
        "    acc_svm_fw = fit_predict(model, X_train_selection, y_train_val, X_test_selection, y_test)\n",
        "    support_svm_fw= X_train.columns[(sfs.get_support())]\n",
        "\n",
        "    ########################\n",
        "    # Model dependent - KNN #\n",
        "    ########################\n",
        "\n",
        "    #fine tuning hyper parameter\n",
        "    knn = KNeighborsClassifier()\n",
        "    parameters_knn = {'n_neighbors' : [1, 5, 10, 20]}\n",
        "\n",
        "    model_knn = GridSearchCV(knn, parameters_knn, cv = 5)\n",
        "    model_knn.fit(X_train_val, y_train_val)\n",
        "\n",
        "    model = KNeighborsClassifier(n_neighbors = model_knn.best_params_['n_neighbors'])\n",
        "    knn = model\n",
        "    acc_knn_100 = fit_predict(model, X_train, y_train, X_test, y_test)\n",
        "\n",
        "\n",
        "    #forward selection\n",
        "    X_train_selection, X_test_selection, sfs = seq_fit_transform(model, X_train_val, y_train_val, X_test, n_best_features)\n",
        "    acc_knn_fw = fit_predict(model, X_train_selection, y_train_val, X_test_selection, y_test)\n",
        "    support_knn_fw= X_train.columns[(sfs.get_support())]\n",
        "\n",
        "    #######################################\n",
        "    # summarizing model dependent result  #\n",
        "    #######################################\n",
        "\n",
        "    acc_100 = [acc_lr_100, acc_rf_100, acc_svm_100, acc_knn_100]\n",
        "    acc_20_fw = [acc_lr_fw, acc_rf_fw, acc_svm_fw, acc_knn_fw]\n",
        "    acc_20_ms = [acc_lr_ms, acc_rf_ms, None, None]\n",
        "\n",
        "    ##################################\n",
        "    # Model independent - Chi Square #\n",
        "    ##################################\n",
        "\n",
        "    model_chi2 = SelectKBest(chi2, k=5)\n",
        "\n",
        "    model_chi2.fit(X_train_val, y_train_val)\n",
        "    support_chi2 = model_chi2.get_support()\n",
        "\n",
        "    X_train_selected_chi2 = model_chi2.transform(X_train_val)\n",
        "    X_test_selected_chi2 = model_chi2.transform(X_test)\n",
        "    chi_support = names[support_chi2]\n",
        "\n",
        "    #train the model with reduce feature\n",
        "    lr3 = lr\n",
        "    acc_lr3 = fit_predict(lr3, X_train_selected_chi2, y_train_val, X_test_selected_chi2, y_test)\n",
        "    rf3 = rf\n",
        "    acc_rf3 = fit_predict(rf3, X_train_selected_chi2, y_train_val, X_test_selected_chi2, y_test)\n",
        "    svm3 = svm\n",
        "    acc_svm3 = fit_predict(svm3, X_train_selected_chi2, y_train_val, X_test_selected_chi2, y_test)\n",
        "    knn3 = knn\n",
        "    acc_knn3 = fit_predict(knn3, X_train_selected_chi2, y_train_val, X_test_selected_chi2, y_test)\n",
        "    acc_chi2 = [acc_lr3, acc_rf3, acc_svm3, acc_knn3]\n",
        "\n",
        "    ########################################\n",
        "    # Model Agnostic - Feature Permutation #\n",
        "    ########################################\n",
        "\n",
        "    #print('Logistic Regression')\n",
        "    lr4 = LogisticRegression()\n",
        "    lr4.fit(X_train_val, y_train_val)\n",
        "    lr4_pi = permutation_importance(lr4, X_train_val, y_train_val,\n",
        "                               n_repeats=30,\n",
        "                               random_state=0)\n",
        "\n",
        "    perm_lr = importance_permutation(lr4_pi,X_train_val,perc_features_selected = perc_best_features)\n",
        "\n",
        "    X_train_selected_lr_perm = X_train_val.loc[:, names[perm_lr]]\n",
        "    X_test_selected_lr_perm = X_test.loc[:, names[perm_lr]]\n",
        "\n",
        "    #train the model with reduced features\n",
        "    lr4 = LogisticRegression()\n",
        "    acc_lr4 = fit_predict(lr4, X_train_selected_lr_perm, y_train_val, X_test_selected_lr_perm, y_test)\n",
        "\n",
        "    #print('\\nRandom Forest')\n",
        "    rf4 = rf\n",
        "    rf4.fit(X_train_val, y_train_val)\n",
        "    rf4_pi = permutation_importance(rf4, X_train_val, y_train_val,\n",
        "                               n_repeats=30,\n",
        "                               random_state=0)\n",
        "\n",
        "    perm_rf = importance_permutation(rf4_pi,X_train_val,perc_features_selected = perc_best_features)\n",
        "\n",
        "    X_train_selected_rf_perm = X_train_val.loc[:, names[perm_rf]]\n",
        "    X_test_selected_rf_perm = X_test.loc[:, names[perm_rf]]\n",
        "\n",
        "    rf4 = rf\n",
        "\n",
        "    acc_rf4 = fit_predict(rf4, X_train_selected_rf_perm, y_train_val, X_test_selected_rf_perm, y_test)\n",
        "\n",
        "    #print('\\nSVM')\n",
        "    svm4 = svm\n",
        "\n",
        "    svm4.fit(X_train_val, y_train_val)\n",
        "    svm4_pi = permutation_importance(svm4, X_train_val, y_train_val,\n",
        "                               n_repeats=30,\n",
        "                               random_state=0)\n",
        "\n",
        "    perm_svm = importance_permutation(svm4_pi,X_train_val,perc_features_selected = perc_best_features)\n",
        "\n",
        "    X_train_selected_svm_perm = X_train_val.loc[:, names[perm_svm]]\n",
        "    X_test_selected_svm_perm = X_test.loc[:, names[perm_svm]]\n",
        "\n",
        "    svm4 = svm\n",
        "\n",
        "    acc_svm4 = fit_predict(svm4, X_train_selected_svm_perm, y_train_val, X_test_selected_svm_perm, y_test)\n",
        "\n",
        "\n",
        "    #print('\\nKNN')\n",
        "    knn4 = knn\n",
        "\n",
        "    knn4.fit(X_train_val, y_train_val)\n",
        "    knn4_pi = permutation_importance(knn4, X_train_val, y_train_val,\n",
        "                               n_repeats=30,\n",
        "                               random_state=0)\n",
        "\n",
        "    perm_knn = importance_permutation(knn4_pi,X_train_val,perc_features_selected = perc_best_features)\n",
        "\n",
        "    X_train_selected_knn_perm = X_train_val.loc[:, names[perm_knn]]\n",
        "    X_test_selected_knn_perm = X_test.loc[:, names[perm_knn]]\n",
        "\n",
        "    knn4 = knn\n",
        "\n",
        "    acc_knn4 = fit_predict(knn4, X_train_selected_knn_perm, y_train_val, X_test_selected_knn_perm, y_test)\n",
        "\n",
        "    acc_perm = [acc_lr4, acc_rf4, acc_svm4, acc_knn4]\n",
        "\n",
        "    ########################################\n",
        "    # Model Independent - Mutual Information  #\n",
        "    ########################################\n",
        "\n",
        "    fs = SelectKBest(score_func=mutual_info_classif, k=n_best_features)\n",
        "    fs.fit(X_train_val, y_train_val)\n",
        "\n",
        "    X_train_mi = SelectKBest(mutual_info_classif, k=6).fit_transform(X_train_val,y_train_val)\n",
        "    X_test_mi = SelectKBest(mutual_info_classif, k=6).fit_transform(X_test,y_test)\n",
        "\n",
        "    #train the model with reduce feature\n",
        "    #print('Logistic Regression')\n",
        "    lr5 = lr\n",
        "    acc_lr_mi = fit_predict(lr5, X_train_mi, y_train_val, X_test_mi, y_test)\n",
        "\n",
        "    #print('Random Forest')\n",
        "    rf5 = rf\n",
        "    acc_rf_mi = fit_predict(rf5, X_train_mi, y_train_val, X_test_mi, y_test)\n",
        "\n",
        "    #print('SVM')\n",
        "    svm5 = svm\n",
        "    acc_svm_mi = fit_predict(svm5, X_train_mi, y_train_val, X_test_mi, y_test)\n",
        "\n",
        "    #print('KNN')\n",
        "    knn5 = knn\n",
        "    acc_knn_mi = fit_predict(knn5, X_train_mi, y_train_val, X_test_mi, y_test)\n",
        "\n",
        "    acc_mi = [acc_lr_mi, acc_rf_mi, acc_svm_mi, acc_knn_mi]\n",
        "\n",
        "    ########################\n",
        "    # Model Agnostic - PCA #\n",
        "    ########################\n",
        "\n",
        "    # Create a dataframe from the transformed data\n",
        "    pca = PCA(n_components=n_best_features)\n",
        "    pca.fit(X_train_val)\n",
        "    X_train_pca = pca.transform(X_train_val)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    X_train_pca_df = pd.DataFrame(X_train_pca, columns=['PC{}'.format(i+1) for i in range(X_train_pca.shape[1])])\n",
        "\n",
        "    #X_test\n",
        "    # Create a dataframe from the transformed data\n",
        "    X_test_pca_df = pd.DataFrame(X_test_pca, columns=['PC{}'.format(i+1) for i in range(X_test_pca.shape[1])])\n",
        "\n",
        "    #train the model with reduce feature\n",
        "    #print('Logistic Regression')\n",
        "    lr6 = lr\n",
        "    acc_lr_pca = fit_predict(lr6, X_train_pca_df, y_train_val, X_test_pca_df, y_test)\n",
        "\n",
        "    #print('Random Forest')\n",
        "    rf6 = rf\n",
        "    acc_rf_pca = fit_predict(rf6, X_train_pca_df, y_train_val, X_test_pca_df, y_test)\n",
        "\n",
        "    #print('SVM')\n",
        "    svm6 = svm\n",
        "    acc_svm_pca = fit_predict(svm6, X_train_pca_df, y_train_val, X_test_pca_df, y_test)\n",
        "\n",
        "    #print('KNN')\n",
        "    knn6 = knn\n",
        "    acc_knn_pca = fit_predict(knn6, X_train_pca_df, y_train_val, X_test_pca_df, y_test)\n",
        "\n",
        "    acc_pca = [acc_lr_pca, acc_rf_pca, acc_svm_pca, acc_knn_pca]\n",
        "\n",
        "    ###############################\n",
        "    # Model Agnostic - Sparse PCA #\n",
        "    ###############################\n",
        "\n",
        "    sparse_pca = SparsePCA(n_components=n_best_features, alpha=0.01)\n",
        "\n",
        "    # fit and transform the data in one step\n",
        "    sparse_pca.fit(X_train_val)\n",
        "    X_train_spca = sparse_pca.transform(X_train_val)\n",
        "    X_test_spca = sparse_pca.transform(X_test)\n",
        "\n",
        "    #X_train\n",
        "    X_train_spca_df = pd.DataFrame(X_train_spca, columns=['PC{}'.format(i+1) for i in range(X_train_spca.shape[1])])\n",
        "\n",
        "    #X_test\n",
        "    X_test_spca_df = pd.DataFrame(X_test_spca, columns=['PC{}'.format(i+1) for i in range(X_test_spca.shape[1])])\n",
        "\n",
        "    #train the model with reduce feature\n",
        "    lr7 = lr\n",
        "    acc_lr_spca = fit_predict(lr7, X_train_spca_df, y_train_val, X_test_spca_df, y_test)\n",
        "\n",
        "    rf7 = rf\n",
        "    acc_rf_spca = fit_predict(rf7, X_train_spca_df, y_train_val, X_test_spca_df, y_test)\n",
        "\n",
        "    svm7 = svm\n",
        "    acc_svm_spca = fit_predict(svm7, X_train_spca_df, y_train_val, X_test_spca_df, y_test)\n",
        "\n",
        "    knn7 = knn\n",
        "    acc_knn_spca = fit_predict(knn7, X_train_spca_df, y_train_val, X_test_spca_df, y_test)\n",
        "\n",
        "    acc_spca = [acc_lr_spca, acc_rf_spca, acc_svm_spca, acc_knn_spca]\n",
        "\n",
        "    ###############################\n",
        "    # Summarizing all result #\n",
        "    ###############################\n",
        "    acc_comp = np.array([acc_100, acc_20_fw, acc_20_ms, acc_chi2, acc_perm, acc_mi,acc_pca, acc_spca])\n",
        "\n",
        "    models = ['lr', 'rf', 'svm', 'knn']\n",
        "    results_accuracy = pd.DataFrame(acc_comp, columns = models, index = ['100%', '20% forward selection', '20% model selection','chi2', 'permutation importance','mutual information','PCA', 'Sparse PCA'])\n",
        "    results_accuracy['file_name'] = file_name\n",
        "\n",
        "    return results_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LbS03aQKFSUE",
      "metadata": {
        "id": "LbS03aQKFSUE"
      },
      "outputs": [],
      "source": [
        "final_result = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6DfBOZEGFSUF",
      "metadata": {
        "id": "6DfBOZEGFSUF"
      },
      "outputs": [],
      "source": [
        "file_url = {'R_NEO_PI.csv' : 'https://drive.google.com/file/d/1XcPEKvV7f0EYuTmHNmqcg1lMBzMHbYdG/view?usp=share_link',\n",
        "            'PRMQ_df.csv' : 'https://drive.google.com/file/d/1AQNixB6CtGyk-H6jVUIcBPJRojKVFCpC/view?usp=share_link',\n",
        "            'PCL5_df.csv' : 'https://drive.google.com/file/d/1KMyTxops8osxDpsL7WY-fF2Fe2JB2AAg/view?usp=share_link',\n",
        "            'NAQ_R.csv' : 'https://drive.google.com/file/d/1u6ChVqTcQZQqpYKIBfoSB0wZHYVqn1le/view?usp=share_link',\n",
        "            'PHQ9_GAD7.csv' : 'https://drive.google.com/file/d/1i6Y2URPTo7GtmOcrkaAxyTy037SMafQv/view?usp=share_link',\n",
        "            'PID5.csv' : 'https://drive.google.com/file/d/1jBToUOA3sUEdPh3Wenk9RHzm1zBQy9ua/view?usp=share_link',\n",
        "            'IESR_df.csv' : 'https://drive.google.com/file/d/1cefqrKIn_C9ym40S3MsLSghTqjKo5rEt/view?usp=share_link',\n",
        "            'RAW_DDDT.csv' : 'https://drive.google.com/file/d/1WrChF-LbVIhOeVTke_ZkWaUGwhEy_4Ys/view?usp=share_link',\n",
        "            'IADQ_DF.csv' : 'https://drive.google.com/file/d/1tJrvQakEJj-fOpDru4gveVHPM6jw7csX/view?usp=share_link',\n",
        "            'DT_df_CC.csv' : 'https://drive.google.com/file/d/1DGibOwzy1sXa9wMmbwyzSAVoyyJhV-H9/view?usp=share_link',\n",
        "            'DT_df_JI.csv' : 'https://drive.google.com/file/d/1-H57PXuri9sKNtk_XpQjX9P3ey1fu54K/view?usp=share_link',\n",
        "            'PRFQ_df.csv' : 'https://drive.google.com/file/d/1CwkrbPaGRSoaX7YpkZj0Vx6rqBZxbjub/view?usp=share_link',\n",
        "            'BF_df_CTU.csv' : 'https://drive.google.com/file/d/15WC2c0SWZ_aQFxOhNWG8JqEDW5aid4cV/view?usp=share_link',\n",
        "            'BF_df_OU.csv' : 'https://drive.google.com/file/d/1gCHDMwsRV2apCbE8r7InYOPZAohxp_sW/view?usp=share_link',\n",
        "            'shortPID5.csv' : 'https://drive.google.com/file/d/19wOlnr9Td2-NWWb51513LPrW_yFbkVE0/view?usp=share_link',\n",
        "            'BF_df_V.csv' : 'https://drive.google.com/file/d/1nfjxBodeLP3cZ0a4LE4nkw4oVYJ02ndu/view?usp=share_link'\n",
        "             }\n",
        "\n",
        "file_name_list = file_url.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fFZBdGBcFSUF",
      "metadata": {
        "id": "fFZBdGBcFSUF"
      },
      "outputs": [],
      "source": [
        "i = 1\n",
        "for file_name in file_name_list:\n",
        "    if file_name in ['R_NEO_PI.csv', 'PID5.csv']:\n",
        "        continue\n",
        "    url = file_url[file_name]\n",
        "    path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "    print(str(i)+\". \"+file_name)\n",
        "\n",
        "    result = repeatability(file_name,path)\n",
        "    final_result = pd.concat([final_result, result])\n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jeGLJrdAN6CP",
      "metadata": {
        "id": "jeGLJrdAN6CP",
        "outputId": "9efd45d7-7ae2-4ea2-f465-57edaa6aa001"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lr</th>\n",
              "      <th>rf</th>\n",
              "      <th>svm</th>\n",
              "      <th>knn</th>\n",
              "      <th>file_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>100%</th>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>DT_df_CC.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20% forward selection</th>\n",
              "      <td>0.77551</td>\n",
              "      <td>0.653061</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>DT_df_CC.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20% model selection</th>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>DT_df_CC.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chi2</th>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.77551</td>\n",
              "      <td>DT_df_CC.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>permutation importance</th>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>DT_df_CC.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chi2</th>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.795918</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.673469</td>\n",
              "      <td>BF_df_V.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>permutation importance</th>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.795918</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>BF_df_V.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mutual information</th>\n",
              "      <td>0.673469</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.591837</td>\n",
              "      <td>0.612245</td>\n",
              "      <td>BF_df_V.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PCA</th>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>BF_df_V.csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sparse PCA</th>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.77551</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>BF_df_V.csv</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>432 rows √ó 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              lr        rf       svm       knn     file_name\n",
              "100%                    0.734694  0.755102  0.755102  0.693878  DT_df_CC.csv\n",
              "20% forward selection    0.77551  0.653061  0.693878  0.714286  DT_df_CC.csv\n",
              "20% model selection     0.755102  0.714286      None      None  DT_df_CC.csv\n",
              "chi2                    0.734694  0.755102  0.734694   0.77551  DT_df_CC.csv\n",
              "permutation importance  0.755102  0.734694  0.734694  0.734694  DT_df_CC.csv\n",
              "...                          ...       ...       ...       ...           ...\n",
              "chi2                    0.734694  0.795918  0.734694  0.673469   BF_df_V.csv\n",
              "permutation importance  0.734694  0.755102  0.795918  0.693878   BF_df_V.csv\n",
              "mutual information      0.673469  0.693878  0.591837  0.612245   BF_df_V.csv\n",
              "PCA                     0.755102  0.734694  0.755102  0.755102   BF_df_V.csv\n",
              "Sparse PCA              0.755102   0.77551  0.755102  0.755102   BF_df_V.csv\n",
              "\n",
              "[432 rows x 5 columns]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46bjviWrFSUF",
      "metadata": {
        "id": "46bjviWrFSUF"
      },
      "source": [
        "## Overal Method analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9Iv-IYdQFSUF",
      "metadata": {
        "id": "9Iv-IYdQFSUF"
      },
      "outputs": [],
      "source": [
        "final_result_2 = final_result.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HbkR22n9FSUF",
      "metadata": {
        "id": "HbkR22n9FSUF"
      },
      "outputs": [],
      "source": [
        "final_result_2 = final_result_2.drop_duplicates(keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oMPTp2DpFSUF",
      "metadata": {
        "id": "oMPTp2DpFSUF",
        "outputId": "95ef0927-0d6a-4ee3-ec7d-cc8cd94e042e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['R_NEO_PI.csv', 'PRMQ_df.csv', 'PCL5_df.csv', 'NAQ_R.csv', 'PHQ9_GAD7.csv', 'PID5.csv', 'IESR_df.csv', 'RAW_DDDT.csv', 'IADQ_DF.csv', 'DT_df_CC.csv', 'DT_df_JI.csv', 'PRFQ_df.csv', 'BF_df_CTU.csv', 'BF_df_OU.csv', 'shortPID5.csv', 'BF_df_V.csv'])"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_name_list =  file_url.keys()\n",
        "file_name_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mGNan1wZFSUF",
      "metadata": {
        "id": "mGNan1wZFSUF"
      },
      "outputs": [],
      "source": [
        "final_result_t = pd.DataFrame()\n",
        "for file_name in file_name_list:\n",
        "    df_new = pd.DataFrame()\n",
        "\n",
        "    dum = final_result_2.loc[final_result_2['file_name'] == file_name]\n",
        "    dum = dum.loc[:, dum.columns != 'file_name']\n",
        "    dum = dum.T\n",
        "    (df_new := dum.unstack().to_frame().T).set_axis(\n",
        "        [f\"{i}_{j}\" for i, j in df_new.columns], axis=1\n",
        "    )\n",
        "\n",
        "    df_new['file_name'] = file_name\n",
        "    final_result_t = pd.concat([final_result_t, df_new])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ju_dCJS-FSUF",
      "metadata": {
        "id": "ju_dCJS-FSUF",
        "outputId": "eb51fd68-ba76-499b-bb4e-44e7f3d3fe99"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"4\" halign=\"left\">100%</th>\n",
              "      <th colspan=\"4\" halign=\"left\">20% forward selection</th>\n",
              "      <th colspan=\"2\" halign=\"left\">20% model selection</th>\n",
              "      <th>...</th>\n",
              "      <th>chi2</th>\n",
              "      <th>file_name</th>\n",
              "      <th colspan=\"4\" halign=\"left\">mutual information</th>\n",
              "      <th colspan=\"4\" halign=\"left\">permutation importance</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>knn</th>\n",
              "      <th>lr</th>\n",
              "      <th>rf</th>\n",
              "      <th>svm</th>\n",
              "      <th>knn</th>\n",
              "      <th>lr</th>\n",
              "      <th>rf</th>\n",
              "      <th>svm</th>\n",
              "      <th>knn</th>\n",
              "      <th>lr</th>\n",
              "      <th>...</th>\n",
              "      <th>svm</th>\n",
              "      <th></th>\n",
              "      <th>knn</th>\n",
              "      <th>lr</th>\n",
              "      <th>rf</th>\n",
              "      <th>svm</th>\n",
              "      <th>knn</th>\n",
              "      <th>lr</th>\n",
              "      <th>rf</th>\n",
              "      <th>svm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.836879</td>\n",
              "      <td>0.865248</td>\n",
              "      <td>0.914894</td>\n",
              "      <td>0.921986</td>\n",
              "      <td>0.879433</td>\n",
              "      <td>0.836879</td>\n",
              "      <td>0.907801</td>\n",
              "      <td>0.893617</td>\n",
              "      <td>None</td>\n",
              "      <td>0.815603</td>\n",
              "      <td>...</td>\n",
              "      <td>0.886525</td>\n",
              "      <td>PRMQ_df.csv</td>\n",
              "      <td>0.907801</td>\n",
              "      <td>0.865248</td>\n",
              "      <td>0.921986</td>\n",
              "      <td>0.900709</td>\n",
              "      <td>0.843972</td>\n",
              "      <td>0.843972</td>\n",
              "      <td>0.886525</td>\n",
              "      <td>0.893617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.878049</td>\n",
              "      <td>0.853659</td>\n",
              "      <td>0.829268</td>\n",
              "      <td>0.902439</td>\n",
              "      <td>0.853659</td>\n",
              "      <td>0.878049</td>\n",
              "      <td>0.878049</td>\n",
              "      <td>0.853659</td>\n",
              "      <td>None</td>\n",
              "      <td>0.926829</td>\n",
              "      <td>...</td>\n",
              "      <td>0.853659</td>\n",
              "      <td>PCL5_df.csv</td>\n",
              "      <td>0.902439</td>\n",
              "      <td>0.682927</td>\n",
              "      <td>0.804878</td>\n",
              "      <td>0.926829</td>\n",
              "      <td>0.829268</td>\n",
              "      <td>0.902439</td>\n",
              "      <td>0.780488</td>\n",
              "      <td>0.878049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.930556</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>None</td>\n",
              "      <td>0.930556</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NAQ_R.csv</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.930556</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.902778</td>\n",
              "      <td>0.944444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.982143</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.982143</td>\n",
              "      <td>0.973214</td>\n",
              "      <td>0.991071</td>\n",
              "      <td>0.982143</td>\n",
              "      <td>None</td>\n",
              "      <td>0.964286</td>\n",
              "      <td>...</td>\n",
              "      <td>0.991071</td>\n",
              "      <td>PHQ9_GAD7.csv</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.991071</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.991071</td>\n",
              "      <td>0.991071</td>\n",
              "      <td>0.991071</td>\n",
              "      <td>0.955357</td>\n",
              "      <td>0.955357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>None</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IESR_df.csv</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.972222</td>\n",
              "      <td>0.972222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.747475</td>\n",
              "      <td>0.69697</td>\n",
              "      <td>0.808081</td>\n",
              "      <td>0.808081</td>\n",
              "      <td>0.787879</td>\n",
              "      <td>0.79798</td>\n",
              "      <td>0.767677</td>\n",
              "      <td>0.767677</td>\n",
              "      <td>None</td>\n",
              "      <td>0.79798</td>\n",
              "      <td>...</td>\n",
              "      <td>0.787879</td>\n",
              "      <td>RAW_DDDT.csv</td>\n",
              "      <td>0.656566</td>\n",
              "      <td>0.59596</td>\n",
              "      <td>0.575758</td>\n",
              "      <td>0.575758</td>\n",
              "      <td>0.787879</td>\n",
              "      <td>0.79798</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.911111</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.844444</td>\n",
              "      <td>0.911111</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>None</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>...</td>\n",
              "      <td>0.844444</td>\n",
              "      <td>IADQ_DF.csv</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.911111</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.77551</td>\n",
              "      <td>0.653061</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>None</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>...</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>DT_df_CC.csv</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.591837</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.734694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.597701</td>\n",
              "      <td>0.609195</td>\n",
              "      <td>0.609195</td>\n",
              "      <td>0.597701</td>\n",
              "      <td>0.494253</td>\n",
              "      <td>0.563218</td>\n",
              "      <td>0.574713</td>\n",
              "      <td>0.528736</td>\n",
              "      <td>None</td>\n",
              "      <td>0.574713</td>\n",
              "      <td>...</td>\n",
              "      <td>0.574713</td>\n",
              "      <td>DT_df_JI.csv</td>\n",
              "      <td>0.517241</td>\n",
              "      <td>0.45977</td>\n",
              "      <td>0.448276</td>\n",
              "      <td>0.45977</td>\n",
              "      <td>0.574713</td>\n",
              "      <td>0.609195</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.586207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.897059</td>\n",
              "      <td>0.897059</td>\n",
              "      <td>0.867647</td>\n",
              "      <td>0.926471</td>\n",
              "      <td>0.882353</td>\n",
              "      <td>0.838235</td>\n",
              "      <td>0.867647</td>\n",
              "      <td>0.852941</td>\n",
              "      <td>None</td>\n",
              "      <td>0.867647</td>\n",
              "      <td>...</td>\n",
              "      <td>0.867647</td>\n",
              "      <td>PRFQ_df.csv</td>\n",
              "      <td>0.852941</td>\n",
              "      <td>0.794118</td>\n",
              "      <td>0.882353</td>\n",
              "      <td>0.794118</td>\n",
              "      <td>0.882353</td>\n",
              "      <td>0.838235</td>\n",
              "      <td>0.882353</td>\n",
              "      <td>0.838235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.911111</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>None</td>\n",
              "      <td>0.644444</td>\n",
              "      <td>...</td>\n",
              "      <td>0.911111</td>\n",
              "      <td>BF_df_CTU.csv</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.844444</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.844444</td>\n",
              "      <td>0.844444</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.804348</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.804348</td>\n",
              "      <td>0.826087</td>\n",
              "      <td>0.826087</td>\n",
              "      <td>0.826087</td>\n",
              "      <td>0.826087</td>\n",
              "      <td>None</td>\n",
              "      <td>0.847826</td>\n",
              "      <td>...</td>\n",
              "      <td>0.804348</td>\n",
              "      <td>BF_df_OU.csv</td>\n",
              "      <td>0.804348</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.804348</td>\n",
              "      <td>0.804348</td>\n",
              "      <td>0.847826</td>\n",
              "      <td>0.717391</td>\n",
              "      <td>0.826087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.932692</td>\n",
              "      <td>0.951923</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.951923</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.932692</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.951923</td>\n",
              "      <td>None</td>\n",
              "      <td>0.932692</td>\n",
              "      <td>...</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>shortPID5.csv</td>\n",
              "      <td>0.951923</td>\n",
              "      <td>0.951923</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.951923</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.932692</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.942308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.77551</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.653061</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>None</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>...</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>BF_df_V.csv</td>\n",
              "      <td>0.612245</td>\n",
              "      <td>0.673469</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.591837</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.795918</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14 rows √ó 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       100%                               20% forward selection            \\\n",
              "        knn        lr        rf       svm                   knn        lr   \n",
              "0  0.836879  0.865248  0.914894  0.921986              0.879433  0.836879   \n",
              "0  0.878049  0.853659  0.829268  0.902439              0.853659  0.878049   \n",
              "0  0.944444  0.944444  0.944444  0.944444              0.944444  0.944444   \n",
              "0  0.982143       1.0       1.0       1.0              0.982143  0.973214   \n",
              "0  0.972222  0.916667  0.972222       1.0              0.972222  0.972222   \n",
              "0  0.747475   0.69697  0.808081  0.808081              0.787879   0.79798   \n",
              "0  0.866667  0.911111  0.866667  0.844444              0.911111  0.866667   \n",
              "0  0.693878  0.734694  0.755102  0.755102              0.714286   0.77551   \n",
              "0  0.597701  0.609195  0.609195  0.597701              0.494253  0.563218   \n",
              "0  0.897059  0.897059  0.867647  0.926471              0.882353  0.838235   \n",
              "0  0.888889  0.911111  0.866667  0.888889              0.733333  0.866667   \n",
              "0  0.782609  0.804348  0.782609  0.804348              0.826087  0.826087   \n",
              "0  0.932692  0.951923  0.942308  0.951923              0.923077  0.932692   \n",
              "0  0.734694   0.77551  0.755102  0.693878              0.693878  0.734694   \n",
              "\n",
              "                      20% model selection            ...      chi2  \\\n",
              "         rf       svm                 knn        lr  ...       svm   \n",
              "0  0.907801  0.893617                None  0.815603  ...  0.886525   \n",
              "0  0.878049  0.853659                None  0.926829  ...  0.853659   \n",
              "0  0.930556  0.944444                None  0.930556  ...       NaN   \n",
              "0  0.991071  0.982143                None  0.964286  ...  0.991071   \n",
              "0  0.972222  0.972222                None  0.972222  ...       NaN   \n",
              "0  0.767677  0.767677                None   0.79798  ...  0.787879   \n",
              "0  0.866667  0.866667                None  0.866667  ...  0.844444   \n",
              "0  0.653061  0.693878                None  0.755102  ...  0.734694   \n",
              "0  0.574713  0.528736                None  0.574713  ...  0.574713   \n",
              "0  0.867647  0.852941                None  0.867647  ...  0.867647   \n",
              "0  0.866667  0.866667                None  0.644444  ...  0.911111   \n",
              "0  0.826087  0.826087                None  0.847826  ...  0.804348   \n",
              "0  0.942308  0.951923                None  0.932692  ...  0.942308   \n",
              "0  0.653061  0.857143                None  0.734694  ...  0.734694   \n",
              "\n",
              "       file_name mutual information                                \\\n",
              "                                knn        lr        rf       svm   \n",
              "0    PRMQ_df.csv           0.907801  0.865248  0.921986  0.900709   \n",
              "0    PCL5_df.csv           0.902439  0.682927  0.804878  0.926829   \n",
              "0      NAQ_R.csv                NaN       NaN       NaN       NaN   \n",
              "0  PHQ9_GAD7.csv                1.0  0.991071       1.0  0.991071   \n",
              "0    IESR_df.csv                1.0  0.972222       1.0       1.0   \n",
              "0   RAW_DDDT.csv           0.656566   0.59596  0.575758  0.575758   \n",
              "0    IADQ_DF.csv           0.866667  0.888889  0.911111  0.888889   \n",
              "0   DT_df_CC.csv           0.693878  0.714286  0.714286  0.591837   \n",
              "0   DT_df_JI.csv           0.517241   0.45977  0.448276   0.45977   \n",
              "0    PRFQ_df.csv           0.852941  0.794118  0.882353  0.794118   \n",
              "0  BF_df_CTU.csv           0.866667  0.844444       0.8  0.844444   \n",
              "0   BF_df_OU.csv           0.804348  0.782609  0.782609  0.804348   \n",
              "0  shortPID5.csv           0.951923  0.951923  0.942308  0.951923   \n",
              "0    BF_df_V.csv           0.612245  0.673469  0.693878  0.591837   \n",
              "\n",
              "  permutation importance                                \n",
              "                     knn        lr        rf       svm  \n",
              "0               0.843972  0.843972  0.886525  0.893617  \n",
              "0               0.829268  0.902439  0.780488  0.878049  \n",
              "0               0.930556  0.944444  0.902778  0.944444  \n",
              "0               0.991071  0.991071  0.955357  0.955357  \n",
              "0               0.944444  0.972222  0.972222  0.972222  \n",
              "0               0.787879   0.79798  0.727273  0.727273  \n",
              "0               0.866667  0.866667  0.866667  0.866667  \n",
              "0               0.734694  0.755102  0.734694  0.734694  \n",
              "0               0.574713  0.609195  0.586207  0.586207  \n",
              "0               0.882353  0.838235  0.882353  0.838235  \n",
              "0               0.844444  0.866667  0.866667  0.866667  \n",
              "0               0.804348  0.847826  0.717391  0.826087  \n",
              "0               0.923077  0.932692  0.923077  0.942308  \n",
              "0               0.693878  0.734694  0.755102  0.795918  \n",
              "\n",
              "[14 rows x 33 columns]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_result_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DsNhzcWwFSUF",
      "metadata": {
        "id": "DsNhzcWwFSUF",
        "outputId": "e8c98e30-4cc0-4145-9078-3f70f05ef0af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100%                    svm    0.859979\n",
              "                        rf     0.851015\n",
              "permutation importance  lr     0.850229\n",
              "20% model selection     rf     0.850102\n",
              "PCA                     svm    0.848719\n",
              "100%                    lr     0.847996\n",
              "PCA                     rf     0.847388\n",
              "20% forward selection   svm    0.846986\n",
              "permutation importance  svm    0.844839\n",
              "20% forward selection   lr     0.843326\n",
              "PCA                     lr     0.841695\n",
              "100%                    knn    0.839671\n",
              "chi2                    rf     0.839276\n",
              "Sparse PCA              rf     0.836688\n",
              "20% forward selection   rf     0.835542\n",
              "chi2                    lr     0.835346\n",
              "PCA                     knn    0.835242\n",
              "Sparse PCA              svm    0.832580\n",
              "permutation importance  knn    0.832240\n",
              "20% model selection     lr     0.830804\n",
              "Sparse PCA              lr     0.830057\n",
              "20% forward selection   knn    0.828440\n",
              "chi2                    svm    0.827758\n",
              "permutation importance  rf     0.825486\n",
              "chi2                    knn    0.825298\n",
              "Sparse PCA              knn    0.819618\n",
              "mutual information      knn    0.817901\n",
              "                        rf     0.805957\n",
              "                        svm    0.793964\n",
              "                        lr     0.785918\n",
              "20% model selection     knn         NaN\n",
              "                        svm         NaN\n",
              "dtype: float64"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#take average accuracy for every method\n",
        "final_result_t_mean = final_result_t.mean(axis=0, skipna= True)\n",
        "final_result_t_mean.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UcOwX4jtFSUF",
      "metadata": {
        "id": "UcOwX4jtFSUF",
        "outputId": "a99025d6-c6e0-4797-f96a-05507e85cfc5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100%                    svm    0.895664\n",
              "PCA                     svm    0.886525\n",
              "100%                    lr     0.881154\n",
              "                        knn    0.872358\n",
              "PCA                     rf     0.872340\n",
              "chi2                    lr     0.870254\n",
              "20% forward selection   rf     0.867157\n",
              "permutation importance  rf     0.866667\n",
              "mutual information      knn    0.866667\n",
              "Sparse PCA              svm    0.866667\n",
              "permutation importance  svm    0.866667\n",
              "100%                    rf     0.866667\n",
              "20% forward selection   knn    0.866546\n",
              "PCA                     lr     0.865248\n",
              "Sparse PCA              lr     0.865248\n",
              "20% forward selection   svm    0.861905\n",
              "chi2                    knn    0.860163\n",
              "20% model selection     rf     0.860163\n",
              "chi2                    rf     0.860163\n",
              "Sparse PCA              rf     0.858156\n",
              "20% model selection     lr     0.857246\n",
              "permutation importance  lr     0.857246\n",
              "20% forward selection   lr     0.852451\n",
              "chi2                    svm    0.849051\n",
              "PCA                     knn    0.844444\n",
              "mutual information      svm    0.844444\n",
              "permutation importance  knn    0.844208\n",
              "Sparse PCA              knn    0.843972\n",
              "mutual information      rf     0.804878\n",
              "                        lr     0.794118\n",
              "20% model selection     knn         NaN\n",
              "                        svm         NaN\n",
              "dtype: float64"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#take median accuracy for every method\n",
        "final_result_t_median = final_result_t.median(axis=0, skipna= True)\n",
        "final_result_t_median.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uSH5NHPOFSUF",
      "metadata": {
        "id": "uSH5NHPOFSUF"
      },
      "source": [
        "Now we are going to analyze the performance of the proposed model.\n",
        "\n",
        "In general, there are 32 combinations of methods that we are proposing. The method with the highest mean and median will be denoted as the most stable. The reason for analyzing both the mean and median is because the mean is somewhat prone to outliers. We want to exclude these particular cases; hence we are investigating the median. However, since we want to find a stable method, we should not overlook edge cases. Hence, we are also considering the mean.\n",
        "\n",
        "The two results above show that maintaining all the features tends to give a better classification performance. Especially when using the SVM algorithm.\n",
        "\n",
        "However, we often deal with a vast dataset with rich features. Therefore, feature selection is necessary to minimize the processing time. The model Selection method paired with the random forest algorithm has the highest mean of accuracy for feature selection with a relatively similar value to its median. Sparse PCA paired with KNN has the highest median accuracy for feature selection. This method tends to work well but may not be a one-stop solution for all.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P3E_OaMpFSUF",
      "metadata": {
        "id": "P3E_OaMpFSUF"
      },
      "source": [
        "## Analysis based on fake good and fake bad datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wY0eG2nRFSUF",
      "metadata": {
        "id": "wY0eG2nRFSUF"
      },
      "outputs": [],
      "source": [
        "#group by fake good fake bad\n",
        "\n",
        "faking_good = {'DT_df_CC.csv' : 'https://drive.google.com/file/d/1DGibOwzy1sXa9wMmbwyzSAVoyyJhV-H9/view?usp=share_link',\n",
        "           'DT_df_JI.csv' : 'https://drive.google.com/file/d/1-H57PXuri9sKNtk_XpQjX9P3ey1fu54K/view?usp=share_link',\n",
        "           'PRFQ_df.csv' : 'https://drive.google.com/file/d/1CwkrbPaGRSoaX7YpkZj0Vx6rqBZxbjub/view?usp=share_link',\n",
        "            'BF_df_CTU.csv' : 'https://drive.google.com/file/d/15WC2c0SWZ_aQFxOhNWG8JqEDW5aid4cV/view?usp=share_link',\n",
        "            'BF_df_OU.csv' : 'https://drive.google.com/file/d/1gCHDMwsRV2apCbE8r7InYOPZAohxp_sW/view?usp=share_link',\n",
        "              'shortPID5.csv' : 'https://drive.google.com/file/d/19wOlnr9Td2-NWWb51513LPrW_yFbkVE0/view?usp=share_link',\n",
        "            'BF_df_V.csv' : 'https://drive.google.com/file/d/1nfjxBodeLP3cZ0a4LE4nkw4oVYJ02ndu/view?usp=share_link',\n",
        "            'R_NEO_PI.csv' : 'https://drive.google.com/file/d/14iQKKDX1LZicize8f5ogn9EzbzjnzC-T/view?usp=share_link'\n",
        "           }\n",
        "\n",
        "faking_bad = {'PRMQ_df.csv' : 'https://drive.google.com/file/d/1AQNixB6CtGyk-H6jVUIcBPJRojKVFCpC/view?usp=share_link',\n",
        "             'PCL5_df.csv' : 'https://drive.google.com/file/d/1KMyTxops8osxDpsL7WY-fF2Fe2JB2AAg/view?usp=share_link',\n",
        "              'NAQ_R.csv' : 'https://drive.google.com/file/d/1dkLpSMhsej9NhqoNWZYQu9huL4j-voXH/view?usp=share_link',\n",
        "              'PHQ9_GAD7.csv' : 'https://drive.google.com/file/d/1i6Y2URPTo7GtmOcrkaAxyTy037SMafQv/view?usp=share_link',\n",
        "              'PID5.csv' : 'https://drive.google.com/file/d/1jBToUOA3sUEdPh3Wenk9RHzm1zBQy9ua/view?usp=share_link',\n",
        "              'IESR_df.csv' : 'https://drive.google.com/file/d/1cefqrKIn_C9ym40S3MsLSghTqjKo5rEt/view?usp=share_link',\n",
        "              'RAW_DDDT.csv' : 'https://drive.google.com/file/d/1WrChF-LbVIhOeVTke_ZkWaUGwhEy_4Ys/view?usp=share_link',\n",
        "              'IADQ_DF.csv' : 'https://drive.google.com/file/d/1tJrvQakEJj-fOpDru4gveVHPM6jw7csX/view?usp=share_link'\n",
        "             }\n",
        "\n",
        "fake_good = faking_good.keys()\n",
        "fake_bad = faking_bad.keys()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V7oGNrdRFSUF",
      "metadata": {
        "id": "V7oGNrdRFSUF"
      },
      "outputs": [],
      "source": [
        "final_result_t_fg = pd.DataFrame()\n",
        "for file_name in fake_good:\n",
        "    df_new = pd.DataFrame()\n",
        "\n",
        "    dum = final_result_2.loc[final_result_2['file_name'] == file_name]\n",
        "    dum = dum.loc[:, dum.columns != 'file_name']\n",
        "    dum = dum.T\n",
        "    (df_new := dum.unstack().to_frame().T).set_axis(\n",
        "        [f\"{i}_{j}\" for i, j in df_new.columns], axis=1\n",
        "    )\n",
        "\n",
        "    df_new['file_name'] = file_name\n",
        "    final_result_t_fg = pd.concat([final_result_t_fg, df_new])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ihwJIwL-FSUG",
      "metadata": {
        "id": "ihwJIwL-FSUG",
        "outputId": "21edef7a-018a-47b6-90dd-bf5d40f0fa24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100%                    lr     0.811977\n",
              "chi2                    rf     0.807887\n",
              "Sparse PCA              lr     0.807320\n",
              "PCA                     lr     0.807320\n",
              "                        rf     0.804594\n",
              "100%                    svm    0.802616\n",
              "Sparse PCA              svm    0.801030\n",
              "PCA                     svm    0.799656\n",
              "chi2                    lr     0.799388\n",
              "Sparse PCA              rf     0.799129\n",
              "20% model selection     rf     0.798957\n",
              "permutation importance  svm    0.798588\n",
              "                        lr     0.797773\n",
              "100%                    rf     0.796947\n",
              "Sparse PCA              knn    0.796769\n",
              "20% forward selection   svm    0.796768\n",
              "chi2                    svm    0.795645\n",
              "20% forward selection   lr     0.791015\n",
              "chi2                    knn    0.790710\n",
              "100%                    knn    0.789646\n",
              "PCA                     knn    0.784479\n",
              "permutation importance  rf     0.780784\n",
              "                        knn    0.779644\n",
              "20% forward selection   rf     0.769078\n",
              "20% model selection     lr     0.765303\n",
              "mutual information      knn    0.757035\n",
              "20% forward selection   knn    0.752467\n",
              "mutual information      rf     0.751958\n",
              "                        lr     0.745803\n",
              "                        svm    0.719754\n",
              "20% model selection     svm         NaN\n",
              "                        knn         NaN\n",
              "dtype: float64"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#take average accuracy for every method\n",
        "final_result_t_fg_mean = final_result_t_fg.mean(axis=0, skipna= True)\n",
        "final_result_t_fg_mean.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KkEkdknUFSUG",
      "metadata": {
        "id": "KkEkdknUFSUG",
        "outputId": "7cd51d2a-07eb-42ae-84da-c0db81861a5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20% forward selection   svm    0.852941\n",
              "permutation importance  lr     0.838235\n",
              "chi2                    rf     0.826087\n",
              "20% forward selection   lr     0.826087\n",
              "                        rf     0.826087\n",
              "permutation importance  svm    0.826087\n",
              "20% model selection     rf     0.826087\n",
              "chi2                    svm    0.804348\n",
              "PCA                     rf     0.804348\n",
              "permutation importance  knn    0.804348\n",
              "chi2                    knn    0.804348\n",
              "100%                    lr     0.804348\n",
              "chi2                    lr     0.804348\n",
              "100%                    svm    0.804348\n",
              "mutual information      knn    0.804348\n",
              "                        svm    0.794118\n",
              "PCA                     lr     0.782609\n",
              "Sparse PCA              lr     0.782609\n",
              "PCA                     knn    0.782609\n",
              "Sparse PCA              rf     0.782609\n",
              "                        svm    0.782609\n",
              "PCA                     svm    0.782609\n",
              "Sparse PCA              knn    0.782609\n",
              "mutual information      rf     0.782609\n",
              "                        lr     0.782609\n",
              "100%                    rf     0.782609\n",
              "                        knn    0.782609\n",
              "20% model selection     lr     0.755102\n",
              "permutation importance  rf     0.755102\n",
              "20% forward selection   knn    0.733333\n",
              "20% model selection     svm         NaN\n",
              "                        knn         NaN\n",
              "dtype: float64"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_result_t_fg_median = final_result_t_fg.median(axis=0, skipna= True)\n",
        "final_result_t_fg_median.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qEeAQyABFSUG",
      "metadata": {
        "id": "qEeAQyABFSUG"
      },
      "source": [
        "Looking at the method performance based on fake good datasets, maintaining all features is the most stable method, especially paired with the Logistic Regression method (mean=81%, median=80%). **The best method for feature selection for fake good is the Chi-square method paired with the random forest algorithm (mean=80,7%, median=82.6%).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6CaeWuqgFSUG",
      "metadata": {
        "id": "6CaeWuqgFSUG"
      },
      "outputs": [],
      "source": [
        "final_result_t_fb = pd.DataFrame()\n",
        "for file_name in fake_bad:\n",
        "    df_new = pd.DataFrame()\n",
        "\n",
        "    dum = final_result_2.loc[final_result_2['file_name'] == file_name]\n",
        "    dum = dum.loc[:, dum.columns != 'file_name']\n",
        "    dum = dum.T\n",
        "    (df_new := dum.unstack().to_frame().T).set_axis(\n",
        "        [f\"{i}_{j}\" for i, j in df_new.columns], axis=1\n",
        "    )\n",
        "\n",
        "    df_new['file_name'] = file_name\n",
        "    final_result_t_fb = pd.concat([final_result_t_fb, df_new])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "svsLfF0ZFSUG",
      "metadata": {
        "id": "svsLfF0ZFSUG",
        "outputId": "cdf5d0e8-00b4-4a84-d99e-a8992b20d23f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100%                    svm    0.917342\n",
              "PCA                     svm    0.905959\n",
              "100%                    rf     0.905082\n",
              "20% forward selection   knn    0.904413\n",
              "permutation importance  lr     0.902685\n",
              "Sparse PCA              rf     0.902418\n",
              "20% forward selection   rf     0.902006\n",
              "20% model selection     rf     0.901248\n",
              "PCA                     rf     0.897314\n",
              "20% forward selection   svm    0.897204\n",
              "20% model selection     lr     0.896306\n",
              "20% forward selection   lr     0.895637\n",
              "PCA                     knn    0.894466\n",
              "permutation importance  svm    0.891090\n",
              "100%                    knn    0.889697\n",
              "mutual information      knn    0.888912\n",
              "Sparse PCA              svm    0.887793\n",
              "chi2                    lr     0.885687\n",
              "permutation importance  knn    0.884837\n",
              "100%                    lr     0.884014\n",
              "chi2                    rf     0.883221\n",
              "PCA                     lr     0.881800\n",
              "mutual information      svm    0.880543\n",
              "chi2                    knn    0.873721\n",
              "                        svm    0.872716\n",
              "permutation importance  rf     0.870187\n",
              "Sparse PCA              lr     0.869847\n",
              "mutual information      rf     0.868955\n",
              "Sparse PCA              knn    0.859604\n",
              "mutual information      lr     0.832720\n",
              "20% model selection     knn         NaN\n",
              "                        svm         NaN\n",
              "dtype: float64"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#take average accuracy for every method\n",
        "\n",
        "final_result_t_fb_mean = final_result_t_fb.mean(axis=0, skipna= True)\n",
        "final_result_t_fb_mean.sort_values(ascending=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ToCIzVbkFSUG",
      "metadata": {
        "id": "ToCIzVbkFSUG",
        "outputId": "024790f5-f0a4-43f2-a806-2ebe8cdb4547"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20% model selection     lr     0.926829\n",
              "100%                    svm    0.921986\n",
              "mutual information      rf     0.916548\n",
              "100%                    rf     0.914894\n",
              "mutual information      svm    0.913769\n",
              "20% forward selection   knn    0.911111\n",
              "100%                    lr     0.911111\n",
              "20% model selection     rf     0.911111\n",
              "20% forward selection   rf     0.907801\n",
              "mutual information      knn    0.905120\n",
              "permutation importance  lr     0.902439\n",
              "Sparse PCA              rf     0.895745\n",
              "PCA                     svm    0.894482\n",
              "20% forward selection   svm    0.893617\n",
              "permutation importance  svm    0.893617\n",
              "chi2                    lr     0.888889\n",
              "permutation importance  rf     0.886525\n",
              "100%                    knn    0.878049\n",
              "20% forward selection   lr     0.878049\n",
              "mutual information      lr     0.877069\n",
              "PCA                     knn    0.876751\n",
              "Sparse PCA              svm    0.876596\n",
              "PCA                     rf     0.875195\n",
              "chi2                    knn    0.866667\n",
              "                        rf     0.866667\n",
              "permutation importance  knn    0.866667\n",
              "PCA                     lr     0.865957\n",
              "Sparse PCA              lr     0.865957\n",
              "chi2                    svm    0.853659\n",
              "Sparse PCA              knn    0.844208\n",
              "20% model selection     knn         NaN\n",
              "                        svm         NaN\n",
              "dtype: float64"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_result_t_fb_median = final_result_t_fb.median(axis=0, skipna= True)\n",
        "final_result_t_fb_median.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kOlvQa7yFSUG",
      "metadata": {
        "id": "kOlvQa7yFSUG"
      },
      "source": [
        "For fake bad dataset, maintaining all the features during the training process is the most stable method, especially using the SVM method (mean=93%, median=94%); this is the same method as the all-time high mean accuracy across all datasets. If we take a closer look, from the median perspective, Sparse PCA paired with the random forest is giving the best performance. Furthermore, its average mean is also relatively high, one of 6 out of 32."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yVQdNPc7FSUG",
      "metadata": {
        "id": "yVQdNPc7FSUG"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "We have conducted several experiments to propose the most stable method to Determine Respondent Honesty in a questionnaire. The stability of this method is a significant metric to consider the applicability of a model. The higher the mean and median method accuracy across all datasets, the more stable the method is.\n",
        "\n",
        "After deep diving into the accuracy performance based on the fake good and fake bad datasets, the top of the 32 proposed methods applied to all datasets, using all the features paired with the SVM algorithm shown to have the most stable accuracy. Hence, our team considers the below procedure can always be applied to any dataset regardless of the class:\n",
        "\n",
        "1. Preprocess dataset\n",
        "2. Train the SVM algorithm with 100% of the features\n",
        "3. Use the trained algorithm to make the prediction on an unseen dataset\n",
        "\n",
        "The above procedure should give a decent result. Our experiment's overall accuracy mean is 86.6%, and the median is 90%.\n",
        "\n",
        "However, we acknowledge that feature selection is sometimes needed to minimize the processing time. Some datasets are too big to be processed if we keep all the available features. To accommodate the need for feature selection, our team considers the below procedure can always be applied to any dataset regardless of the class :\n",
        "\n",
        "1. Preprocess dataset (import, handle missing values, etc.)\n",
        "2. Select the important features that can explain most of the data variance using Sparse PCA\n",
        "3. Train the Random Forest algorithm with the dataset containing only important features from step 2\n",
        "4. Use the trained algorithm to predict on an unseen dataset\n",
        "\n",
        "In the overall dataset we can see that using 20% Model Selection with Random Foreset gives in average the same result as the procedure proposed, but this method can not be paired with all the models.\n",
        "\n",
        "We have tried to distinguish the method accuracy based on the fake good and fake bad dataset categories. In general, the above procedure remains valid for both types. One thing to consider for the fake good dataset, one way to improve the dataset's accuracy is to consider the Chi-Square method for feature selection instead of Sparse PCA.\n",
        "\n",
        "To close this project, we highlight that having a method that works reasonably well with all datasets is tricky. However, these findings can be a good base model for the analyst starting their analysis journey. We understand that it could be overwhelming to consider all the existing methods considering the vast amount of options available. Therefore, our proposed procedure can be taken into consideration as the beginning of something better in the future of the research."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}